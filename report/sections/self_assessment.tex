\section{Self-assesment}
\subsection{Achievements}
The project successfully met the primary objectives defined initially, laying the 
foundation for a reliable, scalable, and easily configurable cluster system. The most 
notable results include:
\begin{itemize}
  \item \textbf{Reliability and fault tolerance}: The adoption of the Raft protocol 
    enabled distributed consensus between nodes, eliminating any single point of failure. 
    This approach ensures that, even in the event of physical failures (e.g., server, 
    network) or software failures (e.g., OS crashes), the cluster continues to operate 
    consistently, maintaining the integrity of the shared state.

  \item \textbf{Dynamic scalability}: The system allows for the addition or removal 
    of nodes without interrupting the services provided. The use of \textit{libvirt} as a node 
    manager abstracted the hypervisor, simplifying the management of virtual machines 
    and easily supporting the scalability of the cluster.

  \item \textbf{Automated configuration}: The creation of bash scripts simplified 
    and automated the environment and system image setup phase. This approach 
    significantly reduces configuration times and ensures consistency in installations.

  \item \textbf{Interaction tools}: The development of CLI tools introduced an intuitive 
    interface for administrators and end users. The CLI allows monitoring the state of
    the cluster, managing dynamic configurations, and sending specific requests, such 
    as retrieving or modifying files.

  \item \textbf{Modular architecture}: The system was designed with a focus on modularity 
    and isolation of components. Dedicated modules, such as \texttt{raft\_log}, \texttt{ConfPool}, and RPC 
    module, resulted in an organized and maintainable system, facilitating future extensions 
    and improvements.

  \item \textbf{Resource optimization}: The choice of Arch Linux as the operating system 
    minimized resource usage, thanks to a lean and customized system image with 
    lightweight components such as \textit{Syslinux} instead of \textit{GRUB}.
\end{itemize}

\subsection{Difficulties}
Initially, the main challenge was managing \textit{libvirt}, particularly in setting up virtual 
machines (VMs) and configuring distinct subnets to separate the public network from the
private one. Due to the complexity of \textit{libvirt}, understanding its intricate mechanisms required 
a steep learning curve. As mentioned earlier, our goal was to create a lean and 
maintainable environment, but achieving this was far from simple.

One of the most challenging aspects was designing and developing a distributed system. 
This task is notoriously difficult, as such systems inherently have complex requirements, 
such as consistency, fault tolerance, and coordination between nodes. This is 
compounded by Raft: translating its principles and guidelines from the paper \cite{1} 
into a concrete solution took considerable time and was simultaneously a mentally demanding challenge.

Furthermore, we aimed to develop a scalable system, with multiple abstraction layers and 
loosely coupled components. This was essential because, as the project grew, managing 
the code became increasingly difficult, leading to growing technical debt. As a result, 
we performed several revisions of the architectural design to address these issues.

Despite our efforts to limit technical debt, it remained a constant challenge. The codebase 
grew vast and complex, making it difficult to manage the goroutines and all aspects related 
to their synchronization, not to mention following the system's flow. Testing also proved 
difficult due to this complexity. We found designing the configuration system particularly 
tricky, as it wasn't immediately clear how to integrate it into the existing system. We 
spent time and effort both rereading the reference paper \cite{1} and restructuring the 
code to arrive at a correct and working solution.

Another significant obstacle was performing testing on the system. Managing multiple virtual 
machines and debugging with simple prints turned out to be cumbersome and time-consuming. 
A distributed logging system would have been a more effective solution, but implementing 
such a system would have required an additional project.

In hindsight, balancing technical complexity while maintaining modularity, scalability, 
and a clean architecture was the greatest challenge we faced throughout the project.

\subsection{Potential improvements}
Despite the significant results, the project has some critical areas for improvement 
that could make the system more robust, efficient, and accessible:
\begin{itemize}
  \item \textbf{Distributed log management}: Currently, debugging and log collection 
    are cumbersome, as they rely on simple print statements. Implementing a 
    distributed logging system would centralize log collection, making it easier 
    to monitor virtual machines and identify issues in real-time.

  \item \textbf{Resolving data races}: A potential data race has been identified, 
    which could cause a race condition within the log commit procedure, where 
    multiple goroutines may access shared resources simultaneously without proper control. 

  \item \textbf{Interface for functionality}: A standardized interface for the plug-and-play 
    integration of modules that determine the functionalities the cluster offers at any given 
    time. Ideally, this would also allow the user to utilize additional modules to enable 
    the cluster to perform various tasks, not just the filesystem function.

  \item \textbf{Elimination of locks}: Various parts of the code include mutexes to avoid 
    race conditions. These systems do not offer strong protection against race conditions 
    and are also extremely inefficient. Removing them with the idea of using lock-free constructs,
    or entirely removing blocking mechanisms would be optimal.

  \item \textbf{Performance optimization}: While the system functions correctly, 
    certain Raft protocol operations could be further optimized. For example, 
    log compression is not currently supported, which can lead to increased 
    resource usage over time. Introducing a snapshot or compression strategy 
    would reduce log space and improve overall performance.

  \item \textbf{Automating production deployment}: While automation has been implemented 
    for initial configuration, deploying the cluster in production environments 
    still requires manual intervention.

  \item \textbf{Graphical user interface (GUI)}: Relying solely on the CLI to interact 
    with the cluster may be inaccessible for less experienced or non-technical users. 
    Developing a simplified graphical interface would allow users to visualize node 
    status, monitor system metrics, and manage configurations with greater intuitiveness.

  \item \textbf{System testing and validation}: The complexity of the system made it
    difficult to run tests and validate functionality. Introducing an automated test 
    suite would be helpful, though it wouldn't be sufficient to guarantee correct behavior.
\end{itemize}
