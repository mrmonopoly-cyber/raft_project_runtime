
\section{PIB}
\subsection{Elementi}
\subsubsection{Nodi}
Come gia' accennato i nodi del cluster sono dell \textbf{Virtual Machine(VM)} create trammite 
\textbf{libvirt} usando un file XML (vm\_creation\/sources\/vm.xml) che ne determina le caratteristiche.
Poiche' in libvirt un file XML detemina la struttura di una specifica VM abbiamo deciso di rendere
tale elemento parametrico aggiungendo i seguenti parametri:
\begin{itemize}
    \item RAFT\_NODE\_NAME: specifica il nome della VM che verra' creata
    \item PATH\_DISK: specifica il disco virtuale che verra' fornito alla VM
\end{itemize}

Viene inoltre precisata il percorso all'immagine di sistema che dovra' usata all'avvio della VM
(\/var\/lib\/libvirt\/images\/raft\_live\_install.iso). Tale immagine e' stata creata e salvata durante la fase 
di configurazione dell'ambiente.


% Sezione vuota per scaletta

\subsubsection{Gestore dei nodi e della rete}
% Sezione vuota per scaletta
Abbiamo progettato l'architettura di rete del \textit{cluste}r con due \textit{subnet} distinte, ciascuna con ruoli specifici per garantire la separazione tra il traffico pubblico rivolto ai \textit{client} e la comunicazione interna del \textit{cluster}:
\begin{itemize}
  \item \textit{Subnet} Pubblica (192.168.2.0/24): Questa \textit{subnet} è basata su \textit{NAT}, il che significa che i nodi nella rete pubblica possono accedere a \textit{internet} esterno e comunicare con sistemi esterni, 
    ma non possono comunicare direttamente tra loro all'interno della \textit{subnet}. Il \textit{pool} di rete pubblica è riservato ai nodi che devono interagire con servizi o client esterni. In questo intervallo, 
    gli indirizzi 1 e 255 sono riservati rispettivamente all'\textit{hypervisor} e ai messaggi di \textit{broadcast}, mentre i restanti indirizzi IP sono assegnati dinamicamente alle macchine virtuali che necessitano di 
    accesso esterno.
  \item \textit{Subnet} Privata (10.0.0.0/24): Questa \textit{subnet} è utilizzata esclusivamente per la comunicazione \textit{intra-cluster}. L'abbiamo creata utilizzando un \textit{bridge} virtuale, che consente a tutti i 
    nodi della \textit{subnet} di comunicare direttamente tra loro. Tuttavia, questa rete non ha accesso a \textit{internet} esterno, assicurando che il suo unico scopo sia facilitare la comunicazione tra i nodi per compiti 
    correlati al \textit{cluster}. Come nella \textit{subnet} pubblica, gli indirizzi 1 e 255 sono riservati, e il resto dell'intervallo è disponibile per i nodi del \textit{cluster}.
\end{itemize}

Separando le \textit{subnet} pubbliche e private, garantiamo che le richieste dei \textit{client} e i messaggi \textit{intra-cluster} siano instradati correttamente senza interferenze. La \textit{subnet} pubblica gestisce 
le interazioni con i \textit{client}, mentre la \textit{subnet} privata è dedicata alle operazioni interne, come il coordinamento e la replica dello stato tra i nodi, come richiesto dal protocollo \textit{Raft}. Questa separazione 
non solo migliora la sicurezza e le prestazioni, ma facilita anche la manutenibilità e la scalabilità del \textit{cluster}.

\subsubsection{Sistema operativo}
% Sezione vuota per scaletta
Nel nostro \textit{cluster}, ogni macchina virtuale usa \textit{Arch Linux} come 
sistema operativo. 
All'avvio, due servizi chiave vengono avviati automaticamente su ogni nodo.
\begin{itemize}
    \item \textit{\textbf{discovery.service}}: Questo processo facilita la scoperta degli indirizzi IP degli altri nodi all'interno della \textit{subnet} 10.0.0.x/24. Lo fa eseguendo scansioni di rete periodiche utilizzando 
        \textit{nmap}. Ogni 30 secondi, il processo di \textit{discovery} recupera e memorizza gli indirizzi IP delle interfacce di rete pubbliche e private degli altri nodi. Questa ricerca continua garantisce che ogni nodo 
        rimanga "consapevole" degli altri nodi all'interno della \textit{subnet}, mantenendo così una comunicazione continua tra loro.
  \item \textit{\textbf{raft\_daemon.service}}: 
        Questo servizio e' responsabile dell'esecuzione del codice che implementa le funzionalita' 
        e la sincronizzazione del cluster. Prima di tutto monta il disco fornito alla VM e lo formatta
        in ext4, questo viene fatto per garantire uno stato consistente ad ogni avvio 
        indipendentemente da cosa possa essere o meno successo prima.
        Viene quindi scaricato trammite \textbf{git clone} la versione gia' compilata del codice.
        Usando questo sitema, in collaborazione con la conformazione dei branch precedentemente
        descritta, permette un aggiornamento dei nodi a caldo senza la necessita' di ricostruire
        le VM da zero. Fatto cio' si verifica se \textbf{discovery.service} ha scoperto un altro 
        nodo oltre a noi stessi, in caso contrario si attende fino alla soddisfazione della condizione.
        La ragione di questa limitazione e' di garantire l'esecuzione del programma solo dopo 
        che e' stata completata almeno una ricerca di altri nodi nella rete.
        Se non venisse garantito cio' potrebbe accadere che un nodo comincia la sua esecuzione senza
        tener conto di chi altro e' presente nella rete.
        Tale limitazione pero' e' solamente apparente in quanto, da protocollo \textbf{RAFT}, 
        un cluster non puo' ritenersi stabile in seguito ai failure a meno che non sia composto da 
        almeno cinque nodi (numero minimo di nodi di un cluster PIB). Se nella rete
        e' presente un solo nodo prevenire la disponibilita' di tale nodo non crea alcun limite
        al protocollo o al cluster stesso. Nel momento in cui nella rete sono presenti almeno due
        nodi \textbf{raft\_daemon.service} eseguira' il programma scaricato dalla repository prima
        citata rendendo quindi disponibile a tutti gli effetti il nodo per il cluster.
\end{itemize}



\subsection{Raft per il consenso distribuito}
\textit{Raft} è un algoritmo di consenso distribuito, ovvero un meccanismo che permette 
a un gruppo di computer (nodi) di raggiungere un accordo su un dato stato, 
anche in presenza di guasti o latenze di rete. In pratica, 
\textit{Raft} assicura che tutti i nodi abbiano una copia identica e 
aggiornata dei dati, garantendo così la coerenza e l'affidabilità di un sistema distribuito.

\subsubsection{Motivazioni}
% Sezione vuota per scaletta
Abbiamo scelto di usare questo protocollo per la gestione del consenso tra i nodi per
le seguenti ragioni:
\begin{itemize}
    \item \textbf{Omogeneita' dei nodi}: 
        come gia' detto nei nostri Obbiettivi il cluster deve avere tutti i nodi uguali, 
        l'utilizzo di RAFT permette di ottenere il risulato richiesto molto semplicemente poiche'
        il protocollo stesso lo prevede.
    \item \textbf{Alta tolleranza ai guasti}: 
        Il cluster, come gia' accennato deve essere privo di \textbf{Single Point Of Failure} e
        tale protocollo, per definizione dello stesso, ne risulta privo. 
        Cosi' facendo garatisce anche un alto livello di affidabilita' da noi ricercato per il 
        cluster.
    \item \textbf{Configurazione dei nodi dinamica}: 
        Il protocollo permette anche di cambiare comfigurazione del cluster senza richiedere
        alcun riavvio o intervento manuale. Per quanto quest'ultima
        parte non sia necessaria per l'applicazione del protocollo, ci permette di garantire
        la scalabilita' pemettendoci di aumentare o diminuire il numero di 
        nodi a nostro piacimento.
\end{itemize}

%\subsubsection{Applicazione}
% Sezione vuota per scaletta
%L'applicazione del protocollo nel cluster risiede all'interno del codice da noi sviluppato
%eseguito da ogni nodo. E' intrinsicamente legato al funzionamento dell'intero ecosistema e, per 
%questa ragione, non e' possibile confinarlo ad unico modulo. 
%Possiamo tuttavia spiegare come e' stato strutturato, a livello di progettazione, all'interno
%del nostro codice e spiegarne il motivo.
%Abbiamo deciso di dividere l'implementazione del protocollo in tre macroaree:
%\begin{itemize}
%    \item RPC: le chiamate remote eseguite tra un nodo e l'altro
%    \item gestore dei log: questo componente si occupa di tutto cio' che riguarda il salvataggio 
%        dei log del protocollo
%    \item gestore della configurazione: tale modulo ha il compito di controllare quali siano 
%        le attuali configurazioni attive e agire opportunamente in base alle situazioni.
%\end{itemize}
%Tralasciando le RPC che sono uno strumento di servizio, i restanti due blocchi non hanno la stessa
%importanza. Inizialmente abbiamo provato a mantenere una sola instanza di log per ogni
%VM. Purtroppo questo approccio si e' rivelato alquanto fallimentare non permettendo
%l'implementazione del cambio di configurazione poiche' ognonuna di esse deve 
%poter avere un log privato.
%Abbiamo quindi deciso di avere un modulo delle configurazioni il quale, al suo interno, contenesse
%il gestore dei log su cui avere controllo.
%Successivamente vengono poi instanziate le diverse configurazioni attive in un suddetto momento
%con accesso, limitato, al log centralizzato e un commit index privato. 
%Cosi' facendo, per aggiornare la disposizione dei nodi, e' sufficente instanziare una
%nuova configurazione, aspettare la migrazione dei nodi per poi eliminare quella vecchia in favore 
%della nuova. Col precedente approccio non sarebbe stato possibile poiche' la migrazione veniva 
%considerata, erroneamente, come una vera e propria disposizione. 
%Per quanto concerne il committing di un log e' sufficente chiedere alle singole configurazioni
%se tale log e' gia' stato processato per poi avere il gestore delle configurazioni che certifica
%il fatto segnandolo nel log centralizzto della macchina virtuale.


\subsubsection{Limiti}
La struttura sopra descritta permette permette di implementare tutte le nostre necessita' 
mantenendo un discreto dissociamento tra le varie componenti ma presenta comunque dei limiti
per nulla irrilevanti:
\begin{itemize}
    \item \textbf{Complessita'}: il sistema descritto risulta molto complesso nell'implementazione 
        e nella comprensione. Per quanto counque permetta un veloce aggiornamento non e' 
        immediato comprenderne il funzionamento per effettuare delle modifiche.
    \item \textbf{Performance}: uno dei principali problemi della nostra applicazione di RAFT 
        e' che, come verra' spiegato meglio nelle prossime sezioni, non risulta molto efficente
        a livello di risorse. 
    \item \textbf{Compressione dei log}: Nonostante non sia necessaria la compressione dei log
        e' alquanto utile ma il nostro utilizzo del protocollo non e' pensato per implementare
        tale funzionalita'. Questo comporta che se fosse necessario implementarla
        non ci sono garanzie che l'attuale struttura sia predisposta.
\end{itemize}

\subsubsection{Implementazione:\\}
La nostra implementazione di Raft è caratterizata da diversi moduli: 
\begin{itemize}
  \item node: definisce la struttura di un nodo Raft, quindi contenente le informazioni riguardanti la connessione al node e il suo indirizzo. Quest'ultimo è rappresentanto grazio a un sottomodulo di node, chiamato address, 
    che contiene la string di numeri separati dal punto (IP) e la porta di riferimento.

  \item localFs: definisce l'astrazione del filesystem locale e le operazioni di scrittura e lettura su di esso. Grazie a questo modulo, salviamo i file localmente sui vari nodi.
  
  \item clusterMetadata: definisce i metadati relativi al cluster e al nodo stesso, come il proprio indirizzo IP e quello del leader (sia publici che privati), il term per tracciare la validità della leadership, il ruolo 
    che ricopre, le informazioni sui timer (election e heartbeat) e i dettagli 
    sull'elezioni come il quorum, il nodo per cui ha votato e se ha il diritto di voto o meno, 
 
  \item raft\_log: definisce il sistema di log di Raft, dove per log intendiamo l'insieme delle entries che rappresentando le operazioni effettuate sul cluster. La prima componente chiave è LogInstance, che 
    rappresenta una singola instanza di una voce nel log di Raft ed è composta dalla entry vera e propria e da un channel che rappresenta il valore di ritorno (il risultato) associato a quell'operazione. L'aggiunta di un 
    di questo channel si è rilevata necessaria per non bloccare il sistema durante l'elaborazione di un risultato alla richiesta; infatti nel momento in cui questo risultato è pronto, esso viene inviato nel channel e ricevuto e 
    utilizzato dal modulo Rpc per preparare il messaggio di risposta contente per l'appunto il risultato dell'operazione. \\
    Il sistem dei log è poi formato da il LogEntrySlave e il LogEntryMaster. Entrambi condividono lo stesso log, ma il primo ha permessi di sola lettura, mentre il secondo ha anche permessi di scrittura; il compito principale 
    del LogEntryMaster è notificare alle LogEntrySlave l'aggiunta di nuove entry, mentre quest'ultima notifica a sua volta il LogEntryMaster quando queste nuove voci sono state commitate, dando il via libera al master per la 
    procedura di commit ufficiale delle entry. \\
    Il sistema è stato concepito per meglio adeguarsi al sistema delle configurazioni e, più precisamente, la ragione dietro alla predisposizione di due log è che le configurazioni da gestire possono essere più di una per volta. 
    Come specificheremo meglio in seguito, il nostro sistema di configurazioni non è altri che un gestore di configurazioni, e mentre le singole possiedono una LogEntrySlave, il gestore principale possiede la LogEntryMaster 
    cosicchè quando le singole configurazioni hanno committato una entry, questo può essere notificato al gestore che provvederà al commit ufficiale, rispettando così il protocollo Raft che specifica che in caso di una situazione
    di join config, una voce si ritiene commitatta quando lo è da entrambe le configurazioni.

  \item rpcs: definisce un'interfaccia generica Rpc, utilizzata in tutto il progetto per astrarre la logica delle varie chiamate RPC nel sistema Raft. Nonostante il nome sia fuorviante, in quanto è acronimo delle Remote Procedure
    Call, ci teniamo a specificare che non abbiamo progettato questo modulo per lavorare in sincrono, bensì in asincrono.
    Per alleggerire il carico di lavoro alla goroutine centrale e avendo un discreto numero di RPC diverse da gestire, abbiamo definito una interfaccia comune per una generica RPC che viene poi implementata da ogni 
    istanza specifica, 
    implementando così una inversione delle dipendenze. Nell'interfaccia in questione sono stati definite solo le funzione che ogni ADT dovrà rispettare per essere considerato una RPC. In particolare sono state 
    definite le seguenti funzioni:
    \begin{itemize}
      \item ToString: Ritorna lo stato del messaggio con i suoi valori in formato di stringa
      \item Execute: Esegue la funzione relativa a quel messaggio utilizzando il payload del messaggio
          come variabili insieme ai metadati del cluster, del log e delle configurazioni attive
      \item Encode: Serializza il messaggio in byte
      \item Decode: Deserializza una serie di byte nel messaggio popolando il messaggio con i dati
        estratti nel processo
    \end{itemize}
    Per eseguire il comportamento previsto da una RPC il modulo fa uso del clusterMetadata 
    per le informazioni sul nodo, i metadati del cluster (serve in particolare per conoscere il numero di nodi nelle configurazioni) e informazioni sul mittente. \\
    Necessitavamo di un sistema il più modulare e scalabile possibile, in quanto,
    nell'eventualità di aggiungere un nuovo tipo di rpc, non avremo dovuto apportare troppe modifiche. A dimostrazione di ciò, oltre alle rpc definite dal protocollo -- AppendEntryRpc, AppendResponse, RequestVoteRPC, 
    RequestVoteResponse -- ne troviamo altre, come l'rpc utilizzato per notificare ai noti che hanno diritto di voto e quelle che definiscono i messaggi che possiamo ricevere dal client. La ragione dietro a questo tipo di 
    progettazione è stata 
    fatta a frutto dei futuri cambiamenti che ci sarebbero stati e che sono stati fatti, difatti il modulo è stato pensando fin dall'inizio in questo modo e, a fronte dei cambiamenti che sono stati fatti, non è per nulla cambiato. 
    La scalabilità è inoltre facilitata dall'utilizzo di uno script e di un rpc template che utiliziamo per generare nuove RPC (garantisce inoltre di evitare eventuali errori di scrittura di nuove rpc). 
    Utiliziamo però un modulo esterno ad rpc per convertire le classi: quando un messaggio arriva, il suoo tipo viene controllato, viene estratto il payload e convertito in base al tipo. Questo rappresenta un 
    debito tecnico (limite), in quanto ad ogni aggiunta di una nuova rpc, bisogna aggiungere un voce nel modulo di conversione. 
    
  \item confPool: gestisce le configurazioni del sistema. Tiene traccia della configurazione principale, che rappresenta lo stato attuale del sistema, e di una o più configurazioni temporanee che potrebbero essere in fase di 
    valutazione o preparazione per diventare la nuova configurazione principale. Inoltre, si assicura che tutti i nodi del sistema siano d'accordo sulla configurazione corrente. Questo è fondamentale per evitare 
    incoerenze e garantire un funzionamento corretto del sistema. \'E l'unico modulo che ha subito numerose ristrutturazioni: inizialmente abbiamo provato a rendere il gestore dei log indipentende dal gestore delle configurazioni, 
    ma tutto cio' si e' rivelato alquanto fallimentare. In particolar modo il cambio di configurazione risultava troppo complesso da implementare e per questo abbiamo deciso di cambiare approccio usando un altro design. Abbiamo 
    quindi deciso di avere un modulo delle configurazioni il quale, al suo interno, contenesse il gestore dei log su cui avere controllo. Così facendo, divenne più semplice gestire molteplici configurazioni, infatti per aggiornare
    la disposizione dei nodi, è sufficente instanziare una nuova configurazione, aspettare la migrazione dei nodi per poi eliminare quella vecchia in favore della nuova. 
    Col precedente approccio non sarebbe stato possibile poiche' la migrazione veniva 
    considerata, erroneamente, come una vera e propria disposizione, mentre adesso è rappresentata dalla coesistenza di due configurazioni. 

\end{itemize}


% Sezione vuota per scaletta


