\usepackage{listings}

\section{PIB}
\subsection{Elementi}
\subsubsection{Nodi}
Come gia' accennato i nodi del cluster sono dell \textbf{Virtual Machine(VM)} create trammite 
\textbf{libvirt} usando il seguente file XML che ne determina le caratteristiche:
\\
\\
\\
\begin{lstlisting}[language=XML]
<domain type='kvm' id='1'>
  <name>RAFT\_NODE\_NAME</name>
  <memory unit='KiB'>1048576</memory>
  <currentMemory unit='KiB'>1048576</currentMemory>
  <vcpu placement='static'>1</vcpu>
  <resource>
    <partition>/machine</partition>
  </resource>
  <os>
    <type arch='x86\_64' machine='pc-q35-8.2'>hvm</type>
  </os>
  <features>
    <acpi/>
    <apic/>
    <vmport state='off'/>
  </features>
  <cpu mode='host-passthrough' check='none' migratable='on'/>
  <clock offset='utc'>
    <timer name='rtc' tickpolicy='catchup'/>
    <timer name='pit' tickpolicy='delay'/>
    <timer name='hpet' present='no'/>
  </clock>
  <on\_poweroff>destroy</on\_poweroff
  <on\_reboot>restart</on\_reboot>
  <on\_crash>destroy</on\_crash>
  <!-- Devices section -->
  <devices>
    <emulator>/usr/bin/qemu-system-x86\_64</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' discard='unmap'/>
      <source file='PATH\_DISK'/>
      <target dev='vda' bus='virtio'/>
      <boot order='1'/>
    </disk>
    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/raft\_live\_install.iso'/>
      <target dev='sda' bus='sata'/>
      <readonly/>
      <boot order='2'/>
    </disk>
    <!-- Other devices omitted for brevity -->
  </devices>
</domain>
\end{lstlisting}

Come precisato nel file di qui sopra ogni nodo e' composto da 1048576KB (1024 MB) di memoria,
ha ua'architettura x86\_64, ha un disco virtuale (file \textbf{.qcow2}) la cui \textbf{PATH} 
non e' specificata in quanto parametrica e determinata al momento dell'allocazione del nodo.
Inoltre a ogni nodo viene associata l'immagine di sistema \textbf{raft\_live\_install.iso} che 
contiene il sistema operativo. il parametro \textbf{RAFT\_NODE\_NAME} e' anch'esso determinato
a tempo di creazione del nodo e per questo risulta, in questo file XML, parametrico.

Prima dell'allocazione di un nodo verranno creati: un XML temporaneo con le caratteristiche 
specifiche del nuovo nodo e un disco virtuale da associare al nodo. 
Una volta generati questi file \textbf{libvirt} creera' una VM seguendo le specifiche del 
nuovo file XML lasciando inalterato l'originale che continuera' ad essere utilizzato come template.

Tale approccio ci permette di generalizzare e personalizzare le specifiche dei nodi garantendone
un semplice e preciso controllo.


% Sezione vuota per scaletta

\subsubsection{Gestore dei nodi e della rete}
% Sezione vuota per scaletta
Abbiamo progettato l'architettura di rete del \textit{cluste}r con due \textit{subnet} distinte, ciascuna con ruoli specifici per garantire la separazione tra il traffico pubblico rivolto ai \textit{client} e la comunicazione interna del \textit{cluster}:
\begin{itemize}
  \item \textit{Subnet} Pubblica (192.168.2.0/24): Questa \textit{subnet} è basata su \textit{NAT}, il che significa che i nodi nella rete pubblica possono accedere a \textit{internet} esterno e comunicare con sistemi esterni, 
    ma non possono comunicare direttamente tra loro all'interno della \textit{subnet}. Il \textit{pool} di rete pubblica è riservato ai nodi che devono interagire con servizi o client esterni. In questo intervallo, 
    gli indirizzi 1 e 255 sono riservati rispettivamente all'\textit{hypervisor} e ai messaggi di \textit{broadcast}, mentre i restanti indirizzi IP sono assegnati dinamicamente alle macchine virtuali che necessitano di 
    accesso esterno.
  \item \textit{Subnet} Privata (10.0.0.0/24): Questa \textit{subnet} è utilizzata esclusivamente per la comunicazione \textit{intra-cluster}. L'abbiamo creata utilizzando un \textit{bridge} virtuale, che consente a tutti i 
    nodi della \textit{subnet} di comunicare direttamente tra loro. Tuttavia, questa rete non ha accesso a \textit{internet} esterno, assicurando che il suo unico scopo sia facilitare la comunicazione tra i nodi per compiti 
    correlati al \textit{cluster}. Come nella \textit{subnet} pubblica, gli indirizzi 1 e 255 sono riservati, e il resto dell'intervallo è disponibile per i nodi del \textit{cluster}.
\end{itemize}

Separando le \textit{subnet} pubbliche e private, garantiamo che le richieste dei \textit{client} e i messaggi \textit{intra-cluster} siano instradati correttamente senza interferenze. La \textit{subnet} pubblica gestisce 
le interazioni con i \textit{client}, mentre la \textit{subnet} privata è dedicata alle operazioni interne, come il coordinamento e la replica dello stato tra i nodi, come richiesto dal protocollo \textit{Raft}. Questa separazione 
non solo migliora la sicurezza e le prestazioni, ma facilita anche la manutenibilità e la scalabilità del \textit{cluster}.

\subsubsection{Sistema operativo}
% Sezione vuota per scaletta
Nel nostro \textit{cluster}, ogni macchina virtuale esegue \textit{Arch Linux} come sistema operativo. All'avvio, due processi chiave vengono avviati automaticamente su ogni nodo.
\begin{itemize}
  \item \textit{\textbf{raft\_daemon.service}}: Questo processo è responsabile dell'esecuzione del codice relativo al protocollo di consenso \textit{Raft}, che garantisce che il nostro sistema distribuito mantenga la tolleranza 
    ai guasti e la consistenza tra tutti i nodi.
  \item \textit{\textbf{discovery.service}}: Questo processo facilita la scoperta degli indirizzi IP degli altri nodi all'interno della \textit{subnet} 10.0.0.x/24. Lo fa eseguendo scansioni di rete periodiche utilizzando 
    \textit{nmap}. Ogni 30 secondi, il processo di \textit{discovery} recupera e memorizza gli indirizzi IP delle interfacce di rete pubbliche e private degli altri nodi. Questa ricerca continua garantisce che ogni nodo 
    rimanga "consapevole" degli altri nodi all'interno della \textit{subnet}, mantenendo così una comunicazione continua tra loro.
\end{itemize}



\subsection{Raft per il consenso distribuito}
\textit{Raft} è un algoritmo di consenso distribuito, ovvero un meccanismo che permette 
a un gruppo di computer (nodi) di raggiungere un accordo su un dato stato, 
anche in presenza di guasti o latenze di rete. In pratica, 
\textit{Raft} assicura che tutti i nodi abbiano una copia identica e 
aggiornata dei dati, garantendo così la coerenza e l'affidabilità di un sistema distribuito.

\subsubsection{Motivazioni}
% Sezione vuota per scaletta
Abbiamo scelto di usare questo protocollo per la gestione del consenso tra i nodi per
le seguenti ragioni:
\begin{itemize}
    \item \textbf{Omogeneita' dei nodi}: 
        come gia' detto nei nostri Obbiettivi il cluster deve avere tutti i nodi uguali, 
        l'utilizzo di RAFT permette di ottenere il risulato richiesto molto semplicemente poiche'
        il protocollo stesso lo prevede.
    \item \textbf{Alta tolleranza ai guasti}: 
        Il cluster, come gia' accennato deve essere privo di \textbf{Single Point Of Failure} e
        tale protocollo, per definizione dello stesso, ne risulta privo. 
        Cosi' facendo garatisce anche un alto livello di affidabilita' da noi ricercato per il 
        cluster.
    \item \textbf{Configurazione dei nodi dinamica}: 
        Il protocollo permette anche di cambiare comfigurazione del cluster senza richiedere
        alcun riavvio o intervento manuale. Per quanto quest'ultima
        parte non sia necessaria per l'applicazione del protocollo, ci permette di garantire
        la scalabilita' pemettendoci di aumentare o diminuire il numero di 
        nodi a nostro piacimento.
\end{itemize}

\end{itemize}


\subsubsection{Applicazione}
% Sezione vuota per scaletta
L'applicazione del protocollo nel cluster risiede all'interno del codice da noi sviluppato
eseguito da ogni nodo. E' intrinsicamente legato al funzionamento dell'intero ecosistema e, per 
questa ragione, non e' possibile confinarlo ad unico modulo. 

Possiamo tuttavia spiegare come e' stato strutturato, a livello di progettazione, all'interno
del nostro codice e spiegarne il motivo.

Abbiamo deciso di dividere l'implementazione del protocollo in tre macroaree:
\begin{itemize}
    \item RPC: le chiamate remote eseguite tra un nodo e l'altro
    \item gestore dei log: questo componente si occupa di tutto cio' che riguarda il salvataggio 
        dei log del protocollo
    \item gestore della configurazione: tale modulo ha il compito di controllare quali siano 
        le attuali configurazioni attive e agire opportunamente in base alle situazioni.
\end{itemize}

Tralasciando le RPC che sono uno strumento di servizio, i restanti due blocchi non hanno la stessa
importanza. Inizialmente abbiamo provato a rendere il gestore dei log controllato o indipentende 
dal geotore delle configurazioni, ma tutto cio' si e' rivelato alquanto fallimentare.
In particolar modo il cambio di configurazione risultava troppo complesso da implementare 
e per questo abbiamo deciso di cambiare approccio usando un altro design. 

Abbiamo quindi deciso di avere un modulo delle configurazioni il quale, al suo interno, contenesse
il gestore dei log su cui avere controllo.
Successivamente vengono poi instanziate le diverse configurazioni attive in un suddetto momento
con accesso, limitato, al log centralizzato.
Cosi' facendo, per aggiornare la disposizione dei nodi, e' sufficente instanziare una
nuova configurazione, aspettare la migrazione dei nodi per poi eliminare quella vecchia in favore 
della nuova. Col precedente approccio non sarebbe stato possibile poiche' la migrazione veniva 
considerata, erroneamente, come una vera e propria disposizione. 
Per quanto concerne il committing di un log e' sufficente chiedere alle singole configurazioni
se tale log e' gia' stato processato per poi avere il gestore delle configurazioni che certifica
il fatto segnandolo nel log locale della macchina virtuale.

\subsubsection{Limiti}
La struttura sopra descritta permette permette di implementare tutte le nostre necessita' 
mantenendo un discreto dissociamento tra le varie componenti ma presenta comunque dei limiti
per nulla irrilevanti:
\begin{itemize}
    \item \textbf{Complessita'}: il sistema descritto risulta molto complesso nell'implementazione 
        e nella comprensione. Per quanto counque permetta un veloce aggiornamento non e' 
        immediato comprenderne il funzionamento per effettuare delle modifiche.
    \item \textbf{Performance}: uno dei principali problemi della nostra applicazione di RAFT 
        e' che, come verra' spiegato meglio nelle prossime sezioni, non risulta molto efficente
        a livello di risorse. 
    \item \textbf{Compressione dei log}: Nonostante non sia necessaria la compressione dei log
        e' alquanto utile ma il nostro utilizzo del protocollo non e' pensato per implementare
        tale funzionalita'. Questo comporta che se fosse necessario implementarla
        non ci sono garanzie che l'attuale struttura sia predisposta.
\end{itemize}
% Sezione vuota per scaletta


