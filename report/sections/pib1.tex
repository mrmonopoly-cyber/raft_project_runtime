
\section{PIB}
\subsection{Elementi}
\subsubsection{Nodi}
Come gia' accennato i nodi del cluster sono dell \textbf{Virtual Machine(VM)} create trammite 
\textbf{libvirt} usando un file XML (vm\_creation\/sources\/vm.xml) che ne determina le caratteristiche.
Poiche' in libvirt un file XML detemina la struttura di una specifica VM abbiamo deciso di rendere
tale elemento parametrico aggiungendo i seguenti parametri:
\begin{itemize}
    \item RAFT\_NODE\_NAME: specifica il nome della VM che verra' creata
    \item PATH\_DISK: specifica il disco virtuale che verra' fornito alla VM
\end{itemize}

Viene inoltre precisata il percorso all'immagine di sistema che dovra' usata all'avvio della VM
(\/var\/lib\/libvirt\/images\/raft\_live\_install.iso). Tale immagine e' stata creata e salvata durante la fase 
di configurazione dell'ambiente.


% Sezione vuota per scaletta

\subsubsection{Gestore dei nodi e della rete}
% Sezione vuota per scaletta
Abbiamo progettato l'architettura di rete del \textit{cluste}r con due \textit{subnet} distinte, ciascuna con ruoli specifici per garantire la separazione tra il traffico pubblico rivolto ai \textit{client} e la comunicazione interna del \textit{cluster}:
\begin{itemize}
  \item \textit{Subnet} Pubblica (192.168.2.0/24): Questa \textit{subnet} è basata su \textit{NAT}, il che significa che i nodi nella rete pubblica possono accedere a \textit{internet} esterno e comunicare con sistemi esterni, 
    ma non possono comunicare direttamente tra loro all'interno della \textit{subnet}. Il \textit{pool} di rete pubblica è riservato ai nodi che devono interagire con servizi o client esterni. In questo intervallo, 
    gli indirizzi 1 e 255 sono riservati rispettivamente all'\textit{hypervisor} e ai messaggi di \textit{broadcast}, mentre i restanti indirizzi IP sono assegnati dinamicamente alle macchine virtuali che necessitano di 
    accesso esterno.
  \item \textit{Subnet} Privata (10.0.0.0/24): Questa \textit{subnet} è utilizzata esclusivamente per la comunicazione \textit{intra-cluster}. L'abbiamo creata utilizzando un \textit{bridge} virtuale, che consente a tutti i 
    nodi della \textit{subnet} di comunicare direttamente tra loro. Tuttavia, questa rete non ha accesso a \textit{internet} esterno, assicurando che il suo unico scopo sia facilitare la comunicazione tra i nodi per compiti 
    correlati al \textit{cluster}. Come nella \textit{subnet} pubblica, gli indirizzi 1 e 255 sono riservati, e il resto dell'intervallo è disponibile per i nodi del \textit{cluster}.
\end{itemize}

Separando le \textit{subnet} pubbliche e private, garantiamo che le richieste dei \textit{client} e i messaggi \textit{intra-cluster} siano instradati correttamente senza interferenze. La \textit{subnet} pubblica gestisce 
le interazioni con i \textit{client}, mentre la \textit{subnet} privata è dedicata alle operazioni interne, come il coordinamento e la replica dello stato tra i nodi, come richiesto dal protocollo \textit{Raft}. Questa separazione 
non solo migliora la sicurezza e le prestazioni, ma facilita anche la manutenibilità e la scalabilità del \textit{cluster}.

\subsubsection{Sistema operativo}
% Sezione vuota per scaletta
Nel nostro \textit{cluster}, ogni macchina virtuale usa \textit{Arch Linux} come 
sistema operativo. 
All'avvio, due servizi chiave vengono avviati automaticamente su ogni nodo.
\begin{itemize}
    \item \textit{\textbf{discovery.service}}: Questo processo facilita la scoperta degli indirizzi IP degli altri nodi all'interno della \textit{subnet} 10.0.0.x/24. Lo fa eseguendo scansioni di rete periodiche utilizzando 
        \textit{nmap}. Ogni 30 secondi, il processo di \textit{discovery} recupera e memorizza gli indirizzi IP delle interfacce di rete pubbliche e private degli altri nodi. Questa ricerca continua garantisce che ogni nodo 
        rimanga "consapevole" degli altri nodi all'interno della \textit{subnet}, mantenendo così una comunicazione continua tra loro.
  \item \textit{\textbf{raft\_daemon.service}}: 
        Questo servizio e' responsabile dell'esecuzione del codice che implementa le funzionalita' 
        e la sincronizzazione del cluster. Prima di tutto monta il disco fornito alla VM e lo formatta
        in ext4, questo viene fatto per garantire uno stato consistente ad ogni avvio 
        indipendentemente da cosa possa essere o meno successo prima.
        Viene quindi scaricato trammite \textbf{git clone} la versione gia' compilata del codice.
        Usando questo sitema, in collaborazione con la conformazione dei branch precedentemente
        descritta, permette un aggiornamento dei nodi a caldo senza la necessita' di ricostruire
        le VM da zero. Fatto cio' si verifica se \textbf{discovery.service} ha scoperto un altro 
        nodo oltre a noi stessi, in caso contrario si attende fino alla soddisfazione della condizione.
        La ragione di questa limitazione e' di garantire l'esecuzione del programma solo dopo 
        che e' stata completata almeno una ricerca di altri nodi nella rete.
        Se non venisse garantito cio' potrebbe accadere che un nodo comincia la sua esecuzione senza
        tener conto di chi altro e' presente nella rete.
        Tale limitazione pero' e' solamente apparente in quanto, da protocollo \textbf{RAFT}, 
        un cluster non puo' ritenersi stabile in seguito ai failure a meno che non sia composto da 
        almeno cinque nodi (numero minimo di nodi di un cluster PIB). Se nella rete
        e' presente un solo nodo prevenire la disponibilita' di tale nodo non crea alcun limite
        al protocollo o al cluster stesso. Nel momento in cui nella rete sono presenti almeno due
        nodi \textbf{raft\_daemon.service} eseguira' il programma scaricato dalla repository prima
        citata rendendo quindi disponibile a tutti gli effetti il nodo per il cluster.
\end{itemize}


\subsection{RPC module}
Nell'implementare il nostro codice l'utilizzo di un sistema di RPC (Remote Procedure Call) e' stato
fondamentale. Vista l'importanza di un tale modulo abbiamo pensato di crearne uno ad hoc per le 
nostre necessita'. Va specificato che l'utilizzo dell'acronimo RPC, in questo caso, è improprio: 
infatti a differenza delle vere e prioprie RPC, che sono sincrone e consentono di chiamare 
delle procedure sulle altre macchine, i nostri messaggi sono asincroni (il mittente non rimane 
in attesa di una risposta) e provocano un comportamento di risposta nel nodo remoto più che un 
chiamata ad una procedura.  
\subsubsection{Obbiettivi::\\}
\begin{itemize}
    \item Fornire una comune interfaccia per tutte le RPC cosi' da poterle trattare tutte 
        allo stesso modo senza dover gestire manualmente ognuna di loro.
    \item Avere un sistema comune per la serializzazione e desirealizzazione dei dati
    \item Introdurre un sistema che permettesse di inserire, modificare, eliminare una o piu' RPC
        senza impattare gravemente nella logica del codice.
\end{itemize}
\subsubsection{Soluzioni:\\}
Per realizzare questo modulo abbiamo ci siamo appoggiati a un meccanismo comunemente utilizzato 
per la serilizzazione degli Abstract Data Type (ADT): \textbf{Protobuf}.
Di questo meccanismo esiste già una implementazione per \textbf{golang}.

Procedendo in ordine con l'elenco sopra definito abbiamo definito una interfaccia comune
a tutti i messaggi delle RPC definendo delle funzioni comuni ad ognuno di essi. In particolare:
\begin{itemize}
  \item ToString: Ritorna lo stato del messaggio con i suoi valori in formato di stringa
  \item Execute: Esegue la funzione relativa a quel messaggio utilizzando il payload 
    del messaggio come variabili insieme ai metadati del cluster, del log e 
    delle configurazioni attive
  \item Encode: Serializza il messaggio in byte
  \item Decode: Deserializza una serie di byte nel messaggio popolando il messaggio 
     con i dati estratti nel processo
\end{itemize}
Inoltre, ogni messaggio è un wrapper del corrispondente messaggio protobuf. 

Per eseguire il comportamento previsto da una RPC il modulo fa uso del clusterMetadata 
per le informazioni sul nodo, i metadati del cluster (serve in particolare per conoscere 
il numero di nodi nelle configurazioni) e informazioni sul mittente. \\
Grazie alla progettazione di un sistema modulare e scalabile, gli obiettivi sono stati raggiunti 
appieno, infatti nel corso dello sviluppo ci siamo imbattuti nella necessità di aggiungere nuovi 
tipi di messaggio. Infatti nella fatispecie del nostro progetto, oltre alle rpc definite dal protocollo 
-- AppendEntryRpc, AppendResponse, RequestVoteRPC, RequestVoteResponse -- ne troviamo altre, 
come l'rpc utilizzato per notificare ai noti che hanno diritto di voto e quelle che definiscono 
i messaggi che possiamo ricevere dal client. Questi bisogni sono emersi durante la fase di 
sviluppo e non prima, e senza alcun dubbio, la strutturazione di un modulo RPC ad hoc 
ha contribuito a rendere queste modifiche poco invasive e impattanti. \\
La scalabilità è inoltre facilitata dall'utilizzo di uno script e di un rpc template 
che utiliziamo per generare nuove RPC (garantisce inoltre di evitare eventuali errori di 
scrittura di nuove rpc). 

Utiliziamo però un modulo esterno ad rpc per convertire le classi: quando un messaggio 
arriva, il suoo tipo viene controllato, viene estratto il payload e convertito in base 
al tipo. Questo rappresenta un debito tecnico (limite), in quanto ad ogni aggiunta di 
una nuova rpc, bisogna aggiungere un voce nel modulo di conversione.


\subsection{Raft per il consenso distribuito}
\textit{Raft} è un algoritmo di consenso distribuito, ovvero un meccanismo che permette 
a un gruppo di computer (nodi) di raggiungere un accordo su un dato stato, 
anche in presenza di guasti o latenze di rete. In pratica, 
\textit{Raft} assicura che tutti i nodi abbiano una copia identica e 
aggiornata dei dati, garantendo così la coerenza e l'affidabilità di un sistema distribuito.

\subsubsection{Motivazioni}
% Sezione vuota per scaletta
Abbiamo scelto di usare questo protocollo per la gestione del consenso tra i nodi per
le seguenti ragioni:
\begin{itemize}
    \item \textbf{Omogeneita' dei nodi}: 
        come gia' detto nei nostri Obbiettivi il cluster deve avere tutti i nodi uguali, 
        l'utilizzo di RAFT permette di ottenere il risulato richiesto molto semplicemente poiche'
        il protocollo stesso lo prevede.
    \item \textbf{Alta tolleranza ai guasti}: 
        Il cluster, come gia' accennato deve essere privo di \textbf{Single Point Of Failure} e
        tale protocollo, per definizione dello stesso, ne risulta privo. 
        Cosi' facendo garatisce anche un alto livello di affidabilita' da noi ricercato per il 
        cluster.
    \item \textbf{Configurazione dei nodi dinamica}: 
        Il protocollo permette anche di cambiare comfigurazione del cluster senza richiedere
        alcun riavvio o intervento manuale. Per quanto quest'ultima
        parte non sia necessaria per l'applicazione del protocollo, ci permette di garantire
        la scalabilita' pemettendoci di aumentare o diminuire il numero di 
        nodi a nostro piacimento.
\end{itemize}

%\subsubsection{Applicazione}
% Sezione vuota per scaletta
%L'applicazione del protocollo nel cluster risiede all'interno del codice da noi sviluppato
%eseguito da ogni nodo. E' intrinsicamente legato al funzionamento dell'intero ecosistema e, per 
%questa ragione, non e' possibile confinarlo ad unico modulo. 
%Possiamo tuttavia spiegare come e' stato strutturato, a livello di progettazione, all'interno
%del nostro codice e spiegarne il motivo.
%Abbiamo deciso di dividere l'implementazione del protocollo in tre macroaree:
%\begin{itemize}
%    \item RPC: le chiamate remote eseguite tra un nodo e l'altro
%    \item gestore dei log: questo componente si occupa di tutto cio' che riguarda il salvataggio 
%        dei log del protocollo
%    \item gestore della configurazione: tale modulo ha il compito di controllare quali siano 
%        le attuali configurazioni attive e agire opportunamente in base alle situazioni.
%\end{itemize}
%Tralasciando le RPC che sono uno strumento di servizio, i restanti due blocchi non hanno la stessa
%importanza. Inizialmente abbiamo provato a mantenere una sola instanza di log per ogni
%VM. Purtroppo questo approccio si e' rivelato alquanto fallimentare non permettendo
%l'implementazione del cambio di configurazione poiche' ognonuna di esse deve 
%poter avere un log privato.
%Abbiamo quindi deciso di avere un modulo delle configurazioni il quale, al suo interno, contenesse
%il gestore dei log su cui avere controllo.
%Successivamente vengono poi instanziate le diverse configurazioni attive in un suddetto momento
%con accesso, limitato, al log centralizzato e un commit index privato. 
%Cosi' facendo, per aggiornare la disposizione dei nodi, e' sufficente instanziare una
%nuova configurazione, aspettare la migrazione dei nodi per poi eliminare quella vecchia in favore 
%della nuova. Col precedente approccio non sarebbe stato possibile poiche' la migrazione veniva 
%considerata, erroneamente, come una vera e propria disposizione. 
%Per quanto concerne il committing di un log e' sufficente chiedere alle singole configurazioni
%se tale log e' gia' stato processato per poi avere il gestore delle configurazioni che certifica
%il fatto segnandolo nel log centralizzto della macchina virtuale.


\subsubsection{Limiti}
La struttura sopra descritta permette permette di implementare tutte le nostre necessita' 
mantenendo un discreto dissociamento tra le varie componenti ma presenta comunque dei limiti
per nulla irrilevanti:
\begin{itemize}
    \item \textbf{Complessita'}: il sistema descritto risulta molto complesso nell'implementazione 
        e nella comprensione. Per quanto counque permetta un veloce aggiornamento non e' 
        immediato comprenderne il funzionamento per effettuare delle modifiche.
    \item \textbf{Performance}: uno dei principali problemi della nostra applicazione di RAFT 
        e' che, come verra' spiegato meglio nelle prossime sezioni, non risulta molto efficente
        a livello di risorse. 
    \item \textbf{Compressione dei log}: Nonostante non sia necessaria, la compressione dei log
        e' alquanto utile ma il nostro utilizzo del protocollo non e' pensato per implementare
        tale funzionalita'. Questo comporta che se fosse necessario implementarla
        non ci sono garanzie che l'attuale struttura sia predisposta.
\end{itemize}

\subsubsection{Implementazione:\\}
Cio' che verra' citato
fa riferimento solamente alla parte di Raft, il resto verra' descritto nella sezione successiva.
Va fatto notare che in tutto il codice non e' mai stato implementato alcuna forma di 
\textbf{SpinLock} o \textbf{BusyWait}, qualora si fosse presentata la necessita' e' stato
creato un loop che veniva sospeso trammite la lettura bloccante di un messaggio su un channel.
Per implementare il protocollo sono stati implementati i seguenti moduli, :
\begin{itemize}
    \item \textbf{raft\_log}: si occupa del salvataggio delle LogEntry e del commitIndex.
    Definisce il sistema di log di Raft, dove per log intendiamo l'insieme delle entries 
    che rappresentando le operazioni effettuate sul cluster. La prima componente chiave è 
    LogInstance, che 
    rappresenta una singola instanza di una voce nel log di Raft ed è composta dalla entry vera 
    e propria e da un channel che rappresenta il valore di ritorno (il risultato) associato a 
    quell'operazione. L'aggiunta di un 
    di questo channel si è rilevata necessaria per non bloccare il sistema durante l'elaborazione 
    di un risultato alla richiesta; infatti nel momento in cui questo risultato è pronto, esso 
    viene inviato nel channel e che inoltrato al nodo corrispondente.
    Il sistem dei log è poi formato dal \textbf{LogEntrySlave} e il \textbf{LogEntryMaster}. 
    Entrambi condividono 
    lo stesso log, ma il primo ha permessi di sola lettura, mentre il secondo ha anche permessi 
    di scrittura; 
    Detto cio' qualora si volesse aggiungere una LogEntry si procede chiamando la funzione
    di \textbf{AppendEntry} che controllera' se la entry non e' gia' presente nel logo, in 
    caso affermativo aggiungera' in coda l'elemento. L'aggiunta di una entry nel log non ha alcuna
    implicazione sul \textbf{CommitIndex} il quale rimane invariato. Durante 
    tutta la fase di inserimento delle entry viene usato un mutex per bloccare race condition
    nell'inserimento di piu' entry allo stesso momento.
    Per incrementare il commmit index esiste una funzione apposita funziona 
    (\textbf{IncreaseCommitIndex}).
    Entrambi queste operazioni sono riservate al \textbf{LogEntryMaster}.
    Lo scopo del \textbf{LogEntrySlave} sara' chiara quando verra' spiegato il 
    cambio di configurazione.

    \item \textbf{nodeIndexPool}: 
        salva gli indirizzi dei nodi attualmente attivi nella rete e anche l'ultimo logEntry
        che hanno ricevuto. Questo modulo e' utilizzato da un nodo solo quando e' il leader
        poiche' gli serve per capire quale deve essere il contenuto delle AppendEntry per ogni nodo.
    \item \textbf{ClusterMetadata}: contiene i metadati del nodo corrente:
        \begin{itemize}
            \item leader ip
            \item ruolo del nodo corrente
            \item diritto di voto del nodo corrente
            \item ip del nodo corrente
            \item term del nodo corrente
            \item durata del timer degli hearthbit 
            \item durata del timer delle elezioni
            \item ultimo voto
        \end{itemize}
        Definisce i metadati relativi al cluster e al nodo stesso, come il proprio indirizzo IP e quello del leader (sia publici che privati), il term per tracciare la validità della leadership, il ruolo 
        che ricopre, le informazioni sui timer (election e heartbeat) e i dettagli 
        sull'elezioni come il quorum, il nodo per cui ha votato e se ha il diritto di voto o meno, 

    \item \textbf{ConfPool}: 
        gestisce le configurazioni del sistema. Tiene traccia della configurazione principale, 
        che rappresenta lo stato attuale del sistema, e di una o più configurazioni temporanee 
        che potrebbero essere in fase di valutazione o preparazione per diventare la nuova 
        configurazione principale. 

        Tale modulo e' l'unico elemento ad avere accesso di scrittura su \textbf{raft\_log},
        lasciando che le instanze delle configurazioni abbiamo permessi di sola lettura.

        La ragione di cio' e' presto detta: 
        Durante una fase di transizione (\textbf{joint configuration}) tra da una configurazione 
        ad un'altra nessuna delle due
        deve essere in grado di prendere decisioni unilaterali sullo stato del cluster autonomamente

        Ergo qualora dovesse essere necessario aggiungere una nuova entry ConfPool aggiungera'
        al log la nuova entry e notifichera' le configurazioni attive che c'e' una nuova entry
        da committare.

        Dopodiche' aspettera' che entrambe le configurazioni siano pronte a committare la entry
        e solo a quel punto verra' realmente incrementato il commitIndex del log. Dato che il commit
        delle entry avviene sempre in ordine di arrivo non e' possibile che una entry venga 
        committata prima di una arrivata prima rispettando l'ordine di arrivo e la consistenza dei
        log tra i nodi del cluster.
        Questa parte e' comune sia per il leader che per i follower, cio' che gli differenzia 
        e' come le singole instanze delle configurazione dichiarano che sono pronti a committare
        una entry nel log in base se il nodo e un Follower o un Leader.

        In particolare:
        \begin{itemize}
            \item Follower: Dichiara la entry committata in automantico non appena ricevuta
                e notifica la ConfPool che tale si puo' committare la prossima entry.
            \item Leader: invia una AppendEntry a tutti i nodi nella configurazione.
                In seguito, quando la maggioranza dei Follower appartente a quella configurazione
                avra' notificato il leader che hanno committato la entry, notifichera' la ConfPool
                che si puo' procedere con il commit.
        \end{itemize}
        Nel caso ci siano due configurazioni attive nello stesso momento, la procedura sopra
        citata viene applicata per ambedue le configurazioni e il commit avviene solo quando
        tutte e due hanno notitificato alla ConfPool che si puo' procedere con il commit.

        La notifica viene effettuata trammite channel a una goroutine della ConfPool che rimane
        in attesa sui channel delle configurazioni prima di incrementare il commitIndex.

        Va fatto notare che quando viene si aggiunge delle LogEntry nel log della ConfPool
        la procedura e' bloccante solo per la parte di aggiunta delle entry nel log, in seguito
        si limita a notificare, trammite un channel, una goroutine interna che si occupa, per 
        ognuna delle entry eggiunte, di continuare con la procedura di commit.
        Cosi' facendo l'aggiunta di una o piu' logEntry non richiede l'attesa del completamento
        della procedura di commit prima di poter continuare con altre operazioni.
        Il risultato e' che la procedura di commit e' completamente asincrona rispetto all'
        inserimento.

        Dopo che un'antry risulta essere pronta per essere committata viene inviato un messagio
        in un altro channel che risveglia una funziona che si occupa di applicare l'entry in 
        questione.

        Il sistema attualmente descritto non e' in grado di committare contemporaneamente 
        piu' log entry nello stesso momento, puo' solo aggiungerne multiple nello stesso momento.
        Va inoltre segnalato che l'utilizzo di un mutex all'interno della procedura 
        di AppendEntry per coordinare gli inserimenti non e' sufficente. Dato infatti che 
        la gestione di ogni RPC ricevuta e' gestita asincronamente da una goroutine
        e' possibile che due o piu' di esse rimangano in attesa sul mutex creando cosi una race
        condition su quale delle due inserisce le sue entry per prime. Per sistemare questo 
        problema servirebbe implementare un sistema con doppia coda di priorita'
        (\textbf{egg shell model}) per mantenere l'ordine di arrivo delle goroutine.
        Purtroppo cio' non e' stato fatto in quanto e' vista da noi come una ottimizzazione da
        implementare in secondo luogo avendo gia' una versiona stabile e funzionante del cluster.

        Nella ConfPool c'e' ancora una goroutine che normalmente e' sospesa da un channel che ne
        interrompe l'esecuzione: \textbf{checkNodeToUpdate}.
        Tale funzione ha lo scopo gestire uno dei casi limiti citati dal protocollo: l'aggiungta
        di un nuovo nodo quando il cluster e' gia' avviato e che quindi deve essere portato a 
        uno stato consitente.

        Per fare cio' ogni qualvolta si vuole inviare una AppendEntry ad un nodo, riconosciuto
        trammite il sup IP, si verifica se il nodo e' presente nella rete. Nel caso non sia presente
        si salve il suo IP in un vettore.
        Quando e se quel nodo entra nella rete viene inviato il suo IP su un channel che risveglia
        la funzione sopra citata. Tale funzione verifica se il nodo che si e' aggiunto deve essere
        aggiornato e in tal caso chiama una RPC con la quale gli toglie il diritto di voto in accordo
        con il protocollo, quando poi quel nodo risulta essere arrivato a uno stato consistente
        il suo diritto di voto viene ripristinato trammite la medesima RPC.

   % \item \textbf{Rpcs}: 
        % definisce un'interfaccia generica Rpc, utilizzata in tutto il progetto per astrarre la 
%        logica delle varie chiamate RPC nel sistema Raft. Nonostante il nome sia fuorviante, 
 %       in quanto è acronimo delle Remote Procedure Call, ci teniamo a specificare che non abbiamo 
 %       progettato questo modulo per lavorare in sincrono, bensì in asincrono.
 %       Per alleggerire il carico di lavoro alla goroutine centrale e avendo un discreto numero di 
 %       RPC diverse da gestire, abbiamo definito una interfaccia comune per una generica RPC che 
 %       viene poi implementata da ogni istanza specifica, 
 %       implementando così una inversione delle dipendenze. 
 %       Nell'interfaccia in questione sono stati definite solo le funzione che ogni ADT dovrà 
 %       rispettare per essere considerato una RPC. In particolare sono state definite le seguenti 
 %       funzioni:
 %       \begin{itemize}
 %           \item ToString: Ritorna lo stato del messaggio con i suoi valori in formato di stringa
 %           \item Execute: Esegue la funzione relativa a quel messaggio utilizzando il payload 
 %               del messaggio come variabili insieme ai metadati del cluster, del log e 
 %               delle configurazioni attive
 %           \item Encode: Serializza il messaggio in byte
 %           \item Decode: Deserializza una serie di byte nel messaggio popolando il messaggio 
 %               con i dati estratti nel processo
 %       \end{itemize}
 %       Per eseguire il comportamento previsto da una RPC il modulo fa uso del clusterMetadata 
 %       per le informazioni sul nodo, i metadati del cluster (serve in particolare per conoscere 
 %       il numero di nodi nelle configurazioni) e informazioni sul mittente. \\
 %       Necessitavamo di un sistema il più modulare e scalabile possibile, in quanto,
 %       nell'eventualità di aggiungere un nuovo tipo di rpc, non avremo dovuto apportare troppe 
 %       modifiche. A dimostrazione di ciò, oltre alle rpc definite dal protocollo 
 %       -- AppendEntryRpc, AppendResponse, RequestVoteRPC, 
 %       RequestVoteResponse -- ne troviamo altre, come l'rpc utilizzato per notificare ai noti 
 %       che hanno diritto di voto e quelle che definiscono i messaggi che possiamo ricevere dal client. 
 %       La ragione dietro a questo tipo di 
 %       progettazione è stata 
 %       fatta a frutto dei futuri cambiamenti che ci sarebbero stati e che sono stati fatti, 
 %       difatti il modulo è stato pensando fin dall'inizio in questo modo e, a fronte dei 
 %       cambiamenti che sono stati fatti, non è per nulla cambiato. 
 %       La scalabilità è inoltre facilitata dall'utilizzo di uno script e di un rpc template 
 %       che utiliziamo per generare nuove RPC (garantisce inoltre di evitare eventuali errori di 
 %       scrittura di nuove rpc). 
 %       Utiliziamo però un modulo esterno ad rpc per convertire le classi: quando un messaggio 
 %       arriva, il suoo tipo viene controllato, viene estratto il payload e convertito in base 
 %       al tipo. Questo rappresenta un debito tecnico (limite), in quanto ad ogni aggiunta di 
 %       una nuova rpc, bisogna aggiungere un voce nel modulo di conversione.

    \item \textbf{CommonMatchIndex}:
        definisce un'interfaccia che si occupa di monitorare l'ultimo indice su cui tutti i nodi 
        sono d'accordo. Per fare questo, esegue periodicamente una serie di controlli per garantire che i 
        nodi siano allineati e che il sistema possa proseguire in modo coerente, inoltre effettua procedure 
        differenti a seconda della capacità di voto di ogni nodo: se il nodo ha diritto di voto, viene 
        una nuova goroutine che verifica ciclicamente aggiornamenti sul proprio matchIndex; in caso contrario, ossia se il 
        nodo non ha diritto di voto, una goroutine parte controllando periodicamente che il proprio matchIndex 
        superi il CommonMatchIndex. Una volta che si verifica questa condizione la gouritine si sposta a 
        controllare aggiornamenti sul matchIndex del nodo in questione. \\
        Per aggiornare il CommonMatchIndex, si controlla che il nuovo matchIndex sia maggiore del CommonMatchIndex 
        e dell'attuale matchIndex del nodo. In caso positivo, l'interfaccia incrementa il numero di nodi "stabili",
        ossia quelli con l'indice aggiornato. Se questo numero supera la metà dei nodi presenti nella configurazione, 
        la entry associata all'indice si può considerare committata e il CommonMatchIndex viene incrementato.\\ 
        Questo sistema è necessario alle singole configurazioni per comunicare al gestore delle configurazioni 
        che hanno committato una nuova entry: in questo modo il gestore delle configurazioni può procedere
        ad incrementare il commit globalmente. Tutto questo procedimento è infine effettuato solamente dal leader 
        per aggiornare il commit index.  
      
\end{itemize}
