
\section{PIB}
\subsection{Elementi}
\subsubsection{Nodi}
Come gia' accennato i nodi del cluster sono dell \textbf{Virtual Machine(VM)} create trammite 
\textbf{libvirt} usando un file XML (vm\_creation\/sources\/vm.xml) che ne determina le caratteristiche.
Poiche' in libvirt un file XML detemina la struttura di una specifica VM abbiamo deciso di rendere
tale elemento parametrico aggiungendo i seguenti parametri:
\begin{itemize}
    \item RAFT\_NODE\_NAME: specifica il nome della VM che verra' creata
    \item PATH\_DISK: specifica il disco virtuale che verra' fornito alla VM
\end{itemize}

Viene inoltre precisata il percorso all'immagine di sistema che dovra' usata all'avvio della VM
(\/var\/lib\/libvirt\/images\/raft\_live\_install.iso). Tale immagine e' stata creata e salvata durante la fase 
di configurazione dell'ambiente.


% Sezione vuota per scaletta

\subsubsection{Gestore dei nodi e della rete}
% Sezione vuota per scaletta
Abbiamo progettato l'architettura di rete del \textit{cluste}r con due \textit{subnet} distinte, ciascuna con ruoli specifici per garantire la separazione tra il traffico pubblico rivolto ai \textit{client} e la comunicazione interna del \textit{cluster}:
\begin{itemize}
  \item \textit{Subnet} Pubblica (192.168.2.0/24): Questa \textit{subnet} è basata su \textit{NAT}, il che significa che i nodi nella rete pubblica possono accedere a \textit{internet} esterno e comunicare con sistemi esterni, 
    ma non possono comunicare direttamente tra loro all'interno della \textit{subnet}. Il \textit{pool} di rete pubblica è riservato ai nodi che devono interagire con servizi o client esterni. In questo intervallo, 
    gli indirizzi 1 e 255 sono riservati rispettivamente all'\textit{hypervisor} e ai messaggi di \textit{broadcast}, mentre i restanti indirizzi IP sono assegnati dinamicamente alle macchine virtuali che necessitano di 
    accesso esterno.
  \item \textit{Subnet} Privata (10.0.0.0/24): Questa \textit{subnet} è utilizzata esclusivamente per la comunicazione \textit{intra-cluster}. L'abbiamo creata utilizzando un \textit{bridge} virtuale, che consente a tutti i 
    nodi della \textit{subnet} di comunicare direttamente tra loro. Tuttavia, questa rete non ha accesso a \textit{internet} esterno, assicurando che il suo unico scopo sia facilitare la comunicazione tra i nodi per compiti 
    correlati al \textit{cluster}. Come nella \textit{subnet} pubblica, gli indirizzi 1 e 255 sono riservati, e il resto dell'intervallo è disponibile per i nodi del \textit{cluster}.
\end{itemize}

Separando le \textit{subnet} pubbliche e private, garantiamo che le richieste dei \textit{client} e i messaggi \textit{intra-cluster} siano instradati correttamente senza interferenze. La \textit{subnet} pubblica gestisce 
le interazioni con i \textit{client}, mentre la \textit{subnet} privata è dedicata alle operazioni interne, come il coordinamento e la replica dello stato tra i nodi, come richiesto dal protocollo \textit{Raft}. Questa separazione 
non solo migliora la sicurezza e le prestazioni, ma facilita anche la manutenibilità e la scalabilità del \textit{cluster}.

\subsubsection{Sistema operativo}
% Sezione vuota per scaletta
Nel nostro \textit{cluster}, ogni macchina virtuale usa \textit{Arch Linux} come 
sistema operativo. 
All'avvio, due servizi chiave vengono avviati automaticamente su ogni nodo.
\begin{itemize}
    \item \textit{\textbf{discovery.service}}: Questo processo facilita la scoperta degli indirizzi IP degli altri nodi all'interno della \textit{subnet} 10.0.0.x/24. Lo fa eseguendo scansioni di rete periodiche utilizzando 
        \textit{nmap}. Ogni 30 secondi, il processo di \textit{discovery} recupera e memorizza gli indirizzi IP delle interfacce di rete pubbliche e private degli altri nodi. Questa ricerca continua garantisce che ogni nodo 
        rimanga "consapevole" degli altri nodi all'interno della \textit{subnet}, mantenendo così una comunicazione continua tra loro.
  \item \textit{\textbf{raft\_daemon.service}}: 
        Questo servizio e' responsabile dell'esecuzione del codice che implementa le funzionalita' 
        e la sincronizzazione del cluster. Prima di tutto monta il disco fornito alla VM e lo formatta
        in ext4, questo viene fatto per garantire uno stato consistente ad ogni avvio 
        indipendentemente da cosa possa essere o meno successo prima.
        Viene quindi scaricato trammite \textbf{git clone} la versione gia' compilata del codice.
        Usando questo sitema, in collaborazione con la conformazione dei branch precedentemente
        descritta, permette un aggiornamento dei nodi a caldo senza la necessita' di ricostruire
        le VM da zero. Fatto cio' si verifica se \textbf{discovery.service} ha scoperto un altro 
        nodo oltre a noi stessi, in caso contrario si attende fino alla soddisfazione della condizione.
        La ragione di questa limitazione e' di garantire l'esecuzione del programma solo dopo 
        che e' stata completata almeno una ricerca di altri nodi nella rete.
        Se non venisse garantito cio' potrebbe accadere che un nodo comincia la sua esecuzione senza
        tener conto di chi altro e' presente nella rete.
        Tale limitazione pero' e' solamente apparente in quanto, da protocollo \textbf{RAFT}, 
        un cluster non puo' ritenersi stabile in seguito ai failure a meno che non sia composto da 
        almeno cinque nodi (numero minimo di nodi di un cluster PIB). Se nella rete
        e' presente un solo nodo prevenire la disponibilita' di tale nodo non crea alcun limite
        al protocollo o al cluster stesso. Nel momento in cui nella rete sono presenti almeno due
        nodi \textbf{raft\_daemon.service} eseguira' il programma scaricato dalla repository prima
        citata rendendo quindi disponibile a tutti gli effetti il nodo per il cluster.
\end{itemize}



\subsection{Raft per il consenso distribuito}
\textit{Raft} è un algoritmo di consenso distribuito, ovvero un meccanismo che permette 
a un gruppo di computer (nodi) di raggiungere un accordo su un dato stato, 
anche in presenza di guasti o latenze di rete. In pratica, 
\textit{Raft} assicura che tutti i nodi abbiano una copia identica e 
aggiornata dei dati, garantendo così la coerenza e l'affidabilità di un sistema distribuito.

\subsubsection{Motivazioni}
% Sezione vuota per scaletta
Abbiamo scelto di usare questo protocollo per la gestione del consenso tra i nodi per
le seguenti ragioni:
\begin{itemize}
    \item \textbf{Omogeneita' dei nodi}: 
        come gia' detto nei nostri Obbiettivi il cluster deve avere tutti i nodi uguali, 
        l'utilizzo di RAFT permette di ottenere il risulato richiesto molto semplicemente poiche'
        il protocollo stesso lo prevede.
    \item \textbf{Alta tolleranza ai guasti}: 
        Il cluster, come gia' accennato deve essere privo di \textbf{Single Point Of Failure} e
        tale protocollo, per definizione dello stesso, ne risulta privo. 
        Cosi' facendo garatisce anche un alto livello di affidabilita' da noi ricercato per il 
        cluster.
    \item \textbf{Configurazione dei nodi dinamica}: 
        Il protocollo permette anche di cambiare comfigurazione del cluster senza richiedere
        alcun riavvio o intervento manuale. Per quanto quest'ultima
        parte non sia necessaria per l'applicazione del protocollo, ci permette di garantire
        la scalabilita' pemettendoci di aumentare o diminuire il numero di 
        nodi a nostro piacimento.
\end{itemize}

\subsubsection{Applicazione}
% Sezione vuota per scaletta
\textit{L'applicazione del protocollo nel cluster risiede all'interno del codice da noi sviluppato
eseguito da ogni nodo. E' intrinsicamente legato al funzionamento dell'intero ecosistema e, per 
questa ragione, non e' possibile confinarlo ad unico modulo. 
Possiamo tuttavia spiegare come e' stato strutturato, a livello di progettazione, all'interno
del nostro codice e spiegarne il motivo.
Abbiamo deciso di dividere l'implementazione del protocollo in tre macroaree:
\begin{itemize}
    \item RPC: le chiamate remote eseguite tra un nodo e l'altro
    \item gestore dei log: questo componente si occupa di tutto cio' che riguarda il salvataggio 
        dei log del protocollo
    \item gestore della configurazione: tale modulo ha il compito di controllare quali siano 
        le attuali configurazioni attive e agire opportunamente in base alle situazioni.
\end{itemize}
Tralasciando le RPC che sono uno strumento di servizio, i restanti due blocchi non hanno la stessa
importanza. Inizialmente abbiamo provato a rendere il gestore dei log indipentende 
dal geotore delle configurazioni, ma tutto cio' si e' rivelato alquanto fallimentare.
In particolar modo il cambio di configurazione risultava troppo complesso da implementare 
e per questo abbiamo deciso di cambiare approccio usando un altro design. 
Abbiamo quindi deciso di avere un modulo delle configurazioni il quale, al suo interno, contenesse
il gestore dei log su cui avere controllo.
Successivamente vengono poi instanziate le diverse configurazioni attive in un suddetto momento
con accesso, limitato, al log centralizzato.
Cosi' facendo, per aggiornare la disposizione dei nodi, e' sufficente instanziare una
nuova configurazione, aspettare la migrazione dei nodi per poi eliminare quella vecchia in favore 
della nuova. Col precedente approccio non sarebbe stato possibile poiche' la migrazione veniva 
considerata, erroneamente, come una vera e propria disposizione. 
Per quanto concerne il committing di un log e' sufficente chiedere alle singole configurazioni
se tale log e' gia' stato processato per poi avere il gestore delle configurazioni che certifica
il fatto segnandolo nel log locale della macchina virtuale.}


\subsubsection{Limiti}
La struttura sopra descritta permette permette di implementare tutte le nostre necessita' 
mantenendo un discreto dissociamento tra le varie componenti ma presenta comunque dei limiti
per nulla irrilevanti:
\begin{itemize}
    \item \textbf{Complessita'}: il sistema descritto risulta molto complesso nell'implementazione 
        e nella comprensione. Per quanto counque permetta un veloce aggiornamento non e' 
        immediato comprenderne il funzionamento per effettuare delle modifiche.
    \item \textbf{Performance}: uno dei principali problemi della nostra applicazione di RAFT 
        e' che, come verra' spiegato meglio nelle prossime sezioni, non risulta molto efficente
        a livello di risorse. 
    \item \textbf{Compressione dei log}: Nonostante non sia necessaria la compressione dei log
        e' alquanto utile ma il nostro utilizzo del protocollo non e' pensato per implementare
        tale funzionalita'. Questo comporta che se fosse necessario implementarla
        non ci sono garanzie che l'attuale struttura sia predisposta.
\end{itemize}
% Sezione vuota per scaletta


