
\section{PIB}
\subsection{Components}
\subsubsection{Nodes}
As previously mentioned, the cluster nodes are Virtual Machines (VMs) created using \textit{libvirt} with an XML file
(\texttt{vm\_creation/} \texttt{sources/vm.xml}) that defines their characteristics.
In \textit{libvirt}, an XML file determines the structure of a specific VM. To make this element flexible, we added 
the following parameters to the XML configuration:
\begin{itemize}
  \item \texttt{RAFT\_NODE\_NAME}: Specifies the name of the VM to be created.
  \item \texttt{PATH\_DISK}: Specifies the virtual disk to be assigned to the VM.
\end{itemize}
Additionally, the path to the system image that must be used when booting the VM is specified as 
\texttt{/var/lib/}\texttt{libvirt/images/}\\ \texttt{raft\_live\_install.iso}. This image was created and saved during the 
environment configuration phase.

\subsubsection{Node and network manager}
We designed the network architecture of the cluster using two distinct subnets, each with specific 
roles to ensure separation between public client-facing traffic and internal cluster communication:
\begin{itemize}
	\item \textbf{Public Subnet (192.168.2.0/24)}:
    This subnet is configured with NAT, meaning nodes in the public network can access the external 
    internet and communicate with external systems but cannot directly communicate with each other within the subnet.
    Within this range, IP addresses 1 and 255 are reserved for the hypervisor and broadcast 
    messages, respectively, while the remaining IPs are dynamically assigned to virtual machines that require external access.

  \item \textbf{Private Subnet (10.0.0.0/24)}:
    This subnet is dedicated exclusively to intra-cluster communication. It is implemented using 
    a virtual bridge, which allows all nodes in the subnet to directly communicate with each other. 
    However, this network has no external internet access, ensuring its sole purpose is to facilitate 
    communication among nodes for cluster-related tasks.
    Similar to the public subnet, IP addresses 1 and 255 are reserved, while the remaining range 
    is available for cluster nodes.
\end{itemize}
By separating the public and private subnets, we ensure that client requests and intra-cluster messages
are routed correctly without interference. The public subnet handles client interactions, while the 
private subnet is used for internal operations, such as coordination and state replication among nodes,
as required by the Raft protocol.
This separation enhances security, performance, maintainability, and scalability of the cluster.

\subsubsection{Operating system}
In our cluster, each virtual machine uses Arch Linux as the operating system. Upon boot, two key 
services are automatically started on each node:
\begin{itemize}
  \item \textbf{\texttt{discovery.service}}:
    This service facilitates the discovery of the IP addresses of other nodes within the 10.0.0.x/24 
    subnet. It performs periodic network scans using nmap. Every 30 seconds, the discovery process 
    retrieves and stores the public and private network interface IPs of other nodes. This continuous 
    scanning ensures that each node remains "aware" of other nodes in the subnet, maintaining seamless communication.

  \item \textbf{\texttt{raft\_daemon.service}}:
    This service is responsible for running the code that implements the cluster's functionality and 
    synchronization. Its main tasks include:
    \begin{itemize}
      \item Mounting and formatting the disk provided to the VM as \texttt{ext4}. This ensures a consistent
        state at every boot, regardless of previous events.
      \item Cloning the precompiled code version from the repository via git clone. This approach, 
        combined with the branch structure previously described, allows for hot updates to nodes 
        without requiring VM reconstruction.
      \item Checking if \texttt{discovery.service} has detected at least one other node in the network. 
        If no other node is found, the service waits until the condition is met. This ensures 
        that the program only executes after at least one network scan has been completed.
    \end{itemize}
    The last task seems a limitation, but it's crucial to prevent a node from starting execution 
    without considering the presence of other nodes in the network. However, this restriction is 
    not impactful, as the Raft protocol requires a cluster to have at least five nodes to ensure
    stability following failures (the minimum number for a PIB cluster). 
    If only one node exists in the network, delaying its availability does not limit the protocol
    or the cluster. Once at least two nodes are present, \texttt{raft\_daemon.service} executes the program 
    fetched from the repository, making the node fully available to the cluster.
\end{itemize}


\subsection{RPC Module} \label{RPCmodule}
The implementation of an RPC (Remote Procedure Call) system was a fundamental aspect of our project. 
Given the importance of this module, we developed a custom solution tailored to our specific needs. 
It is worth noting that the use of the acronym RPC here is somewhat improper: unlike traditional RPCs, 
which are synchronous and allow procedures to be invoked on other machines, our messages are 
asynchronous (the sender does not wait for a response).

The choice to make messages asynchronous seemed immediately natural. We provide a simple case we 
believe sufficiently illustrates why we chose asynchrony over synchrony: a node failure. If a node 
is waiting for 
a response from another offline node, the wait could potentially be infinite, 
preventing it from making progress. A possible solution could be introducing a timeout, but on what 
basis should it be set? Even with a time limit, the issue would persist in the case of slow nodes: 
a sender could overload a slow receiver, making it even slower.

Although these messages trigger a response behavior 
on the remote node rather than directly invoking a procedure, we chose to retain the term RPC 
to maintain consistency with the terminology in the reference article \cite{1}.

\subsubsection{Goals:\\}
The primary objectives of our RPC module were as follows:
\begin{itemize}
	\item Provide a common interface for all RPCs to enable uniform handling without requiring individual management for each message type.
	\item Implement a standardized system for data serialization and deserialization.
	\item Introduce a system that allows adding, modifying, or removing one or more RPCs without significantly impacting the code logic.
\end{itemize}

\subsubsection{Solutions:\\}
To build this module, we relied on a widely used mechanism for serializing Abstract Data Types 
(ADTs): \textit{Protobuf}. An existing implementation of this mechanism was utilized \cite{3}.

The following steps outline our approach to achieving the defined goals:
\begin{itemize}
	\item \textbf{Common interface for RPC messages}:
    We created a unified interface that applies to all RPC messages, defining a set of 
    core functions for each message:
    	\begin{itemize}
        	\item ToString: Returns the state of the message and its values in string format.
        	\item Execute: Executes the behavior defined by the RPC. To achieve this, the 
            module leverages cluster metadata for node information, cluster configurations 
            (e.g., the number of nodes), and sender details.
        	\item Encode: Serializes the message into bytes.
        	\item Decode: Deserializes a byte array into a message, populating the message with the extracted data.
	    \end{itemize}

    	\item \textbf{Scalability and modularity}:
    The modular design of the system proved highly effective in meeting our objectives. 
    During development, we encountered the need to add new message types beyond those 
    defined by the Raft protocol (e.g., AppendEntryRpc, AppendResponse, RequestVoteRpc, 
    RequestVoteResponse). Examples include RPCs used to notify nodes with voting rights 
    or to handle messages received from the client.
    These requirements emerged during development rather than during initial planning, 
    and the structured design of the custom RPC module ensured that such additions 
    were minimally invasive and low-impact.

    	\item \textbf{Automated generation of new RPCs}:
    Scalability was further enhanced through the use of a script and an RPC template, 
    which allowed for the generation of new RPCs. This approach also helped prevent 
    potential errors when creating new message types.

    	\item \textbf{External conversion module}:
    Since each RPC is wrapped within a generic message, 
        every incoming message is first deserialized into a structured format 
    containing a TYPE and a PAYLOAD.
    The TYPE indicates which RPC was received and determines the decoding method to apply.
    Once the type is verified, the payload is extracted and converted into the appropriate internal format.

    This design was necessary because it is not possible to automatically convert a raw 
    byte message directly into its corresponding Protobuf message. Each Protobuf message 
    has its own decoding method.

    While we acknowledge this as a technical debt, since adding a new RPC requires updating 
    the conversion module, we deemed it essential to establish a general format understood 
    by all nodes. Once the generic message is received, each node proceeds to extract and 
    internally process the payload.
\end{itemize}

\subsection{Raft for distributed consensus}
Raft is a distributed consensus algorithm designed to enable a group of computers (nodes) 
to agree on a shared state, even in the presence of faults or network latency. In essence, 
Raft ensures that all nodes maintain an identical and up-to-date copy of the data, thereby 
guaranteeing the consistency and reliability of a distributed system.

\subsubsection{Motivations}
We chose this protocol to manage consensus among the nodes for the following reasons:
\begin{itemize}
	\item \textbf{Homogeneity of nodes}:
    As outlined in our objectives, the cluster must consist of identical nodes. Raft 
    facilitates this requirement seamlessly, as the protocol inherently assumes and 
    enforces uniformity among nodes.
	\item \textbf{High fault tolerance}:
    The cluster is designed to avoid any single point of failure. Raft, by its 
    very definition, is free of such vulnerabilities, making it a highly reliable choice 
    for our cluster. This feature aligns with our goal of achieving a fault-tolerant and 
    dependable distributed system.
	\item  \textbf{Dynamic node configuration}:
    Raft supports dynamic cluster reconfiguration without requiring restarts or manual 
    intervention. While this capability is not strictly essential for protocol application, 
    it significantly enhances scalability, allowing us to flexibly increase or 
    decrease the number of nodes in the cluster as needed.
\end{itemize}

\subsubsection{Limitations}
The structure described above allows us to implement all our needs while maintaining a 
reasonable level of decoupling between components. However, it presents some significant limitations:
\begin{itemize}
    \item \textbf{Complexity}: The described system is highly complex in both implementation 
      and comprehension. While it allows for rapid updates, understanding its workings 
      for making modifications is not straightforward.
    \item \textbf{Performance}: One of the primary issues with our Raft application is that, 
      as will be explained in later sections, it is not resource-efficient.
    \item \textbf{Log Compression}: Although not strictly necessary, log compression is 
      highly useful. However, our use of the protocol does not account for this functionality. 
      This means that if compression needs to be implemented in the future, there is no 
      guarantee that the current structure will support it.
\end{itemize}
\vspace{3pt}
\subsubsection{Implementation}
The following discussion pertains exclusively to the Raft component. Other aspects are 
described in subsequent sections.

It should be noted that no form of \textbf{SpinLock} or \textbf{BusyWait} was implemented 
in the code. Instead, we created loops that wait on different channels and 
perform different computations depending on the message received. This choice was driven 
by the presence of multiple goroutines in the system, which execute concurrently and 
communicate with each other through channels. Some of these goroutines suspend themselves 
on different channels, as they may expect different types of messages. An example of 
this approach is the part called "\textit{loop}" in Section \ref{operation}.

The Raft protocol implementation comprises the following modules:
\begin{itemize}
  \item \textbf{\texttt{raft\_log}}:
    Handles the storage of \texttt{LogEntry} objects and the \texttt{commitIndex}.
    This module defines the Raft logging system, where the "log" represents the 
    collection of entries recording the operations executed in the cluster. The key 
    component is the \texttt{LogInstance}, which represents a single log entry. It consists 
    of the actual entry and a channel used to return the result of the associated 
    operation. Adding this channel was necessary to avoid blocking the system while 
    processing a request's result. Once the result is ready, it is sent through the 
    channel to the corresponding node.

    The logging system also includes the \textbf{\texttt{LogEntrySlave}} and \textbf{\texttt{LogEntryMaster}}. Both share
    the same log, but the former has read-only permissions, while the latter has 
    both read and write permissions.

    When adding a new log entry, the \textbf{\texttt{AppendEntry}} function checks whether the entry
    already exists in the log. If not, it appends the entry to the log. Adding a 
    log entry does not affect the \textbf{\texttt{commitIndex}}, which remains unchanged. During 
    the entire insertion process, a mutex is used to prevent data races caused 
    by concurrent entry insertions.
    
    The \texttt{\textbf{IncreaseCommitIndex}} function is used to increment the commit index, and 
    both operations are restricted to the \textbf{\texttt{LogEntryMaster}}. The purpose of \textbf{\texttt{LogEntrySlave}}
    will become clearer when discussing configuration changes.

  \item \textbf{\texttt{nodeIndexPool}}: This module stores the addresses of active nodes in the network 
    and tracks the last \texttt{LogEntry} they received. It is used only by the leader to determine 
    the content of \texttt{AppendEntry} messages for each node.

  \item \textbf{\texttt{ClusterMetadata}}: This module describes both the cluster and the node itself, 
    such as the node's IP address, the leader's IP (both public and private), the term used
    to track leadership validity, the node's role, timer information (election and heartbeat),
    and details about the last election, including quorum, the node for which it voted, 
    and its voting rights.

  \item \textbf{\texttt{ConfPool}}: This module manages system configurations. It tracks the main 
    configuration, which represents the current system state, as well as one or more temporary 
    configurations that might be under evaluation or preparation to become the new main configuration.
  
    This module is the only component with write access to \textbf{\texttt{raft\_log}}, while configuration 
    instances are granted read-only permissions.
  
    The reason for this is straightforward: during a transition phase (\textbf{joint configuration}) 
    from one configuration to another, neither configuration should be allowed to make 
    unilateral decisions about the cluster's state independently. Therefore, if a new \texttt{ConfPool}
    entry needs to be added, the module will append the new 
    entry to the log and notify the active configurations that there is a new entry ready to be committed.

    Afterwards, it will wait for both configurations to be ready to commit the entry. Only
    then will the \texttt{commitIndex} of the log be incremented. Since entries are always committed
    in the order they are received, it is impossible for an entry to be committed before an
    earlier one, ensuring both arrival order and log consistency across the cluster nodes.

    This process is the same for both the leader and followers. The distinction lies in how
    each instance of the configuration declares readiness to commit an entry, depending on
    whether the node is a Follower or a Leader.
    
    Specifically:
    \begin{itemize}
      \item Follower: Automatically marks the entry as committed as soon as it is received
        and notifies the ConfPool that the next entry can be committed.
      \item Leader: Sends an AppendEntry to all nodes in the configuration. Once a majority
        of the followers in that configuration notify the leader that they have committed 
        the entry, the leader will notify the \texttt{ConfPool} that the commit can proceed.
    \end{itemize}

    If two configurations are active simultaneously, the procedure described above is
    applied to both configurations. The commit only occurs when both configurations notify
    the \texttt{ConfPool} that it can proceed.
    Notifications are sent via channels to a \texttt{ConfPool} goroutine, which waits on configuration
    channels before incrementing the commitIndex.

    It should be noted that when \texttt{LogEntry} entries are added to the \texttt{ConfPool} log, the process
    blocks only for the part where the entries are added to the log. Subsequently, it 
    simply notifies, via a channel, an internal goroutine that handles the commit procedure 
    for each added entry.
    This way, adding one or more \texttt{LogEntry} entries does not require waiting for the commit 
    procedure to complete before continuing with other operations. As a result, the commit
    procedure is entirely asynchronous with respect to the entry insertion.
    Once an entry is ready to be committed, a message is sent on another channel to wake up 
    a function responsible for applying the entry in question.

    Currently, the described system cannot commit multiple log entries simultaneously; 
    it can only add multiple entries simultaneously. Additionally, using a mutex within 
    the \texttt{AppendEntry} procedure to coordinate insertions is not sufficient.

    Since the handling of each received RPC is managed asynchronously by a goroutine, two 
    or more goroutines might end up waiting on the mutex, creating a race condition over 
    which one inserts its entries first.
    To solve this issue, a synchronization avoidance mechanism is needed to maintain the 
    arrival order of the goroutines. Unfortunately, this has not been implemented yet, 
    as it is considered an optimization to be addressed later, given that the cluster 
    already has a stable and functional version.

    Within the \texttt{ConfPool}, there is another goroutine that is typically suspended, waiting
    on a channel: \textbf{checkNodeToUpdate}. This function is responsible for handling one of 
    the edge cases mentioned in the protocol: the addition of a new node to the cluster 
    after it has already started, ensuring the new node reaches a consistent state.
    To achieve this, whenever an \texttt{AppendEntry} needs to be sent to a node identified by 
    its IP, the system verifies if the node is present on the network. If the node 
    is not present, its IP is stored in a vector. When and if the node joins the network, 
    its IP is sent to a channel that wakes up the function mentioned above. This 
    function verifies whether the newly added node needs to be updated and, if so, 
    calls an RPC to revoke its voting rights, as per the protocol. Once the node 
    reaches a consistent state, its voting rights are restored using the same RPC. 

  \item \textbf{CommonMatchIndex}: This defines an interface responsible for monitoring
    the last index on which all nodes agree. To achieve this, it periodically performs 
    a series of checks to ensure that the nodes are aligned and that the system can 
    proceed consistently. Additionally, it carries out different procedures depending 
    on each node's voting rights:
    \begin{itemize}
      \item If a node has voting rights, a new goroutine is launched to periodically 
        check for updates to its matchIndex.
      \item If a node does not have voting rights, a goroutine periodically checks 
        whether its \texttt{matchIndex} has surpassed the \texttt{CommonMatchIndex}. Once this condition
        is met, the goroutine shifts to monitoring updates to the \texttt{matchIndex} of the corresponding node.
    \end{itemize}
    To update the \texttt{CommonMatchIndex}, the system verifies that the new \texttt{matchIndex} is 
    greater than both the current\\ \texttt{CommonMatchIndex} and the node's current \texttt{matchIndex}. If 
    this condition is met, the interface increments the count of "stable" nodes, i.e., 
    nodes with an updated index. 
    If the count of stable nodes exceeds half the nodes in the configuration, the entry
    associated with the index can be considered committed, and the \texttt{CommonMatchIndex} is incremented.

    This system is essential for individual configurations to notify the configuration 
    manager that they have committed a new entry. This allows the configuration manager 
    to proceed with incrementing the global commit index. 

    Finally, this entire procedure is performed exclusively by the leader to update the commit index.
\end{itemize}

In this implementation, several RPCs are defined using the module described in \ref{RPCmodule}. 
Their definitions are shown below.
\begin{itemize}
  \item \textbf{AppendEntryRpc}: This is sent by the leader on two occasions: when it needs
    to send a heartbeat signal to the follower nodes or when there are entries in the log 
    that still need to be replicated.
    When a follower receives this message, it first checks the sender’s term. If the term 
    is lower, the replication operation fails, and it sends an AppendResponse with a negative 
    result. If the term passes the check, the node changes its role to follower and sets 
    leaderIp to the sender’s address. This is necessary for two main reasons:
    \begin{itemize}
      \item Only the leader can send AppendEntryRpc, so the sender must be the leader.
      \item There is a possibility that the receiver is also a leader but of a lower 
        term (an outdated leader), which must be demoted to follower. This situation 
        could occur if the old leader went offline and a new election took place, 
        resulting in an increased term for the remaining nodes.
    \end{itemize}
    Afterwards, the node checks the length of the entries vector received to determine 
    if the message is a heartbeat message or an attempt by the leader to replicate its log. 
    In the latter case, it performs a consistency check that may fail under the following 
    circumstances:
    \begin{itemize}
      \item The follower has fewer entries than the leader, excluding the new entries 
        (in this case, the leader must decrement the \texttt{nextIndex} and \texttt{matchIndex} for this 
        node and retry the send).
      \item At the same position (index), the follower and leader have different terms. 
    \end{itemize}
    If either of these conditions occurs, the node responds with false, including 
    the index of the inconsistent entry. Otherwise, it responds with true, indicating success.
        
    Finally, the node resets its election timeout.

  \item \textbf{AppendEntryResponse}: This is sent by each follower node to the leader 
    in response to an AppendEntryRpc. The leader checks the result of the AppendEntryRpc:
    \begin{itemize}
      \item If the result is positive, it increments the node’s \texttt{nextIndex} and \texttt{matchIndex} parameters.
     \item If the result is negative, there are two possible reasons: either the leader’s 
       term is lower than the sender’s term, in which case the leader adjusts its term and 
        reverts to the follower state; or there is a log inconsistency with the sender node, 
        in which case the leader adjusts the corresponding \texttt{nextIndex}. This ensures that in the 
        next AppendEntryRpc, the leader will resolve the inconsistency.
    \end{itemize}

  \item \textbf{VoteRequest}: This is sent by any node whose election timer has expired to request votes 
    from other nodes. When a node receives this, it performs three checks:
    \begin{itemize}
      \item The sender’s term is at least equal to the receiver’s term.
      \item The receiver has either already voted for the sender or has not voted at all.
      \item The index and term of the sender’s last log entry are greater than or equal to those of the receiver.
      \item If all these conditions are met, the receiver grants its vote to the sender; otherwise, it does not.
    \end{itemize}

  \item \textbf{VoteResponse}: This is the response nodes send after receiving a vote request. 
      Depending on the outcome of the request:
      \begin{itemize}
        \item If successful, the receiver increments the number of supporters.
        \item If unsuccessful, it decrements the number of supporters.
      \end{itemize}
      The node calculates the majority threshold. If the number of supporters surpasses 
      the threshold, it changes its role to leader. Otherwise, it remains a follower and 
      resets its election parameters (supporter count and the candidate it voted for).

  \item \textbf{UpdateNode}: This is used by the leader to grant voting rights to a follower node.

  \item \textbf{ClientRequest}: This represents a message received by the leader from a client. The 
    leader translates this RPC into a log entry, adds it to its log, and waits for the 
    cluster to process it. Once processed, the result is sent back to the client. Each 
    log entry is associated with a return channel for the result. The goroutine handling 
    the RPCs waits for this result on the channel.

  \item \textbf{ClientReturnValue}: This represents the message containing the result 
    of a ClientRequest that the leader sends back to the client.

  \item \textbf{NewConf}: This message is sent by the operator to declare the initial 
    configuration of the cluster. 
\end{itemize}
