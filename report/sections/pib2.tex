\subsection{Implementation details of the application}
Now that the structure of the project is clear, we can proceed with the description 
of the code and its corresponding functionalities. The programming language chosen 
for this implementation is \textbf{Golang} due to its simplicity, its user-space threads 
(better known as \textbf{goroutines}), and its communication channels \cite{4}.

To avoid unnecessary explanations about how the Raft protocol works, we will assume 
that the reader is familiar with the topic. Otherwise, we recommend referring to 
the relevant paper \cite{1}.

\subsubsection{Goal}
Using the previously mentioned elements to define the structure of the cluster, 
the objectives of this part of the project are:
\begin{itemize}
  \item Implement the Raft protocol to synchronize the nodes.
  \item Handle requests sent by clients, distinguishing them from those sent by other nodes.
  \item Recognize when a new node is added to the network.
  \item Implement configuration changes according to the criteria of the Raft protocol.
  \item Detect when a node becomes inactive.
  \item Enable the simultaneous handling of one or more of the aforementioned requests without implementing spinlocks.
\end{itemize}

\subsubsection{Operation}
The code is structured into two phases:
\begin{itemize}
  \item \textbf{Initialization}: In this phase, all the IPs discovered by the \texttt{discovery.service}
    daemon (the node's own IP and those of other nodes currently in the network) and the 
    directory for saving distributed filesystem data are loaded. Once this is done, the node 
    attempts to establish a connection with each discovered IP address. Additionally, during 
    this phase, clocks required for implementing the Raft protocol are instantiated, although 
    they remain inactive until the node becomes part of a configuration.
  \item \textbf{Loop}: In this phase, a loop is executed with a blocking call that releases the 
    CPU until one of three conditions occurs:
    \begin{itemize}
      \item The node receives an RPC request.
      \item The timer responsible for electing a new leader expires.
      \item The timer signaling the need to send a new heartbeat expires.
    \end{itemize}   
    It is important to note that whenever one of these conditions is met, a goroutine is 
    created to handle the request, freeing up the main loop. This approach ensures better 
    response times while maintaining service availability.
\end{itemize}
The explanation of the code continues by investigating how new connections are established 
outside the initialization phase, how a node recognizes its membership in a specific configuration, 
and the three loop cases mentioned above, explaining the modules used step by step. Each 
of the conditions involves receiving a message in a specific channel.

\paragraph{Establishing a New Connection}
As mentioned earlier, during the initialization phase, each node attempts to connect to all 
nodes discovered by its daemon.
After that, a goroutine is launched to listen for new connections. Every time a new connection 
arrives, the node checks the IP address, distinguishes between "internal" (node) and "external"
(client) addresses, launches a goroutine for each category, and waits to receive messages 
from these connections.

The distinction between requests is made based on the source of the request:
\begin{itemize}
  \item \textbf{192.168.2.X}: External node (client). If the node receiving the request is 
    the server, it processes the request; if it is a follower, it returns the master's 
    address to the client.
  \item \textbf{10.0.0.X}: Internal node. The connection is saved in the node's memory 
    if it is not already present.
\end{itemize}
Using this system, each new internal node communicates its existence to all currently 
existing nodes. If it is already part of the network, it will be contacted by the new nodes, 
thus establishing a connection.

\paragraph{Establishing connections between nodes and configuration membership}
Initially, each node waits to be informed of its membership in a specific configuration. 
This can occur in different ways:
\begin{itemize}
  \item The current master establishes a connection with the next node and sends an 
    AppendEntryRpc with the relevant configuration. The node becomes a follower and 
    applies the received AppendEntryRpc, activating the cluster configuration.
  \item The system administrator sends a message to the node specifying its 
    configuration membership. When this happens, the node becomes the master. 
    This situation only occurs during the creation of a new cluster and is necessary 
    to define the first master to start the service.
\end{itemize}
When either of these situations occurs, the previously disabled timers are activated.

\paragraph{a node receives a RPC request}
As previously mentioned, each node is assigned a goroutine responsible for receiving 
messages from that connection. When a message is received, it is entirely deserialized 
using Protobuf.

After extracting the message, the procedure retrieves the sender's IP address, which 
is necessary to determine where to send a response if required. The RPC is executed, 
and if the procedure returns a response to be sent back, the response is sent, and a 
notification is made on a dedicated channel for that message.
This last part was introduced to ensure that a node cannot saturate the leader with 
messages and monopolize it. This design decision means that the leader will not accept 
additional messages from the same node until the previous message has been processed. 
We are aware that this is a significant limitation, but it was the best solution we 
could find to ensure execution correctness without introducing race conditions in the 
handling of messages from a single node.

\paragraph{election timer trigger}
As soon as the election timer signals its expiration on its channel, the main loop 
starts a goroutine called \texttt{startElection}. This function signals the beginning of a 
new election and resets some internal variables, such as the number of supporters 
and dissenters, preparing the node for the leadership competition.
Next, \texttt{startElection} increments the node's current term, indicating that the node 
is attempting to become the leader in a new election cycle. This operation is critical 
to ensure that all nodes recognize the progress of the election and acknowledge that 
the node attempting to become the leader has a higher "term" than the other nodes, 
which is a fundamental requirement for the validity of a vote request.

An important step is the self-vote. The node votes for itself, expressing its intention 
to become the leader. This is a key step in the Raft protocol, where every node 
must attempt to gain consensus from other nodes.
After voting for itself, the node creates an RPC message for the vote request, 
preparing to send it to the other nodes in the cluster.

Finally, startElection sends this vote request to all nodes in the cluster. By using 
a goroutine for each message sent, the function ensures that the requests are sent in 
parallel, thus optimizing the waiting time and increasing the chances of receiving quick responses.

\subsubsection{heartbeat timer trigger}
When the heartbeat timer expires, another goroutine is launched, iterating through 
all the IP addresses of the nodes listed in the configuration. For each IP address, 
the function verifies whether the node exists (i.e., it is reachable within the 
subnet). If the node is found, the function retrieves the current state of that node.
This step is crucial because it provides updated information, such as the index of the 
next log entry to process. With the node identified and the necessary information 
retrieved, the function determines whether the node requires new log entries. If the 
nextIndex of the node is less than the total number of available log entries, it 
indicates there are updates to be sent.
In such cases, a message of type AppendEntryRpc is created. This message creation is 
fundamental to ensuring that the followers remain synchronized with the leader. If no 
new log entries need to be sent, a generic heartbeat message is generated. This 
heartbeat acts as a "keep-alive" signal, demonstrating that the leader is still active and operational.
Once the heartbeat message is created, the function encodes it into a format suitable 
for transmission over the network. Finally, the encoded message is sent to the node. 
This process is repeated for all elements present in the configuration.

\subsubsection{File System sbstraction}
As outlined in the objectives, the cluster implements a distributed file system.
In this context, it was deemed essential to define a level of abstraction for the 
file system on each node to simplify interactions and ensure a more modular and 
manageable implementation.

The file system abstraction for each node represents a standardized interface that 
enables uniform file operations regardless of the specifics of the node. This approach 
ensures a high level of consistency and simplicity in managing the distributed file system.

At the core of this abstraction is a single primary method called \texttt{ApplyLogEntry}, which 
plays a central role in processing file operations. This method is designed to accept 
a log entry as input and handle the following functions:
\begin{itemize}
  \item Analyze the log entry to identify the type of operation to be performed, such as: 
    creating a new file, writing to a file and renaming, deleting a file
  \item Execute the corresponding operation on the node's file system.
\end{itemize}


\subsubsection{Flaws}
Although our system works as described in the document \cite{1}, it has a significant flaw.
\begin{lstlisting}[language=Go]
func (c *commonMatchImp) 
  for c.run { 
    if newMatch > c.commonMatchIndex && 
        sub.Trd < newMatch { 
      c.numStable++ 
      if c.numStable > uint(halfNodeNum) {
        if c.commitEntryC != nil {
          c.commitEntryC <- c.commonMatchIndex
        } 
        c.commonMatchIndex++
        c.numStable = 1
      } 
    } 
    sub.Trd = newMatch
  } 
}
\end{lstlisting}
The issue stems from a critical section of the code that is accessed simultaneously
by multiple goroutines without any access control mechanism. Specifically, it is 
possible for one goroutine to pass the initial check and execute the intended operations, 
only for a second goroutine to pass the same check immediately afterward. At this point, 
however, the first goroutine has already modified the shared values within the body 
of the if statement. This means the second goroutine operates on outdated values, 
effectively invalidating the guard condition.

Even though we conducted numerous tests (which we acknowledge are insufficient to 
prove the absence of bugs) and found it challenging to reproduce the necessary 
conditions to expose race conditions, we have not yet observed this issue in practice.
