\subsection{Dettagli implementativi dell'applicativo}
Ora che e' chiara la struttura del progetto si puo' procedere con la descrizione del
codice e delle relative funzionalita'. Per fare cio' e' stato scelto come linguaggio
di programmazione \textbf{Golang} per la sua semplicita' d'uso e per i thread in user space
meglio note come \textbf{goroutine}.
Per non dilungarsi nella inutile spiegazione di come funziona il protocollo RAFT d'ora in avanti
si assumera' che il lettore abbia conoscenza dell'argomento. In caso contrario si invita
a leggere il paper di riferimento.
\subsubsection{Obiettivi}
Avendo usato gli elementi precendentemente citati per delineare la struttura del cluster tutto
cio' che deve fare questa parte del progetto e':
\begin{itemize}
    \item implementare il protocollo RAFT per sincronizzare i nodi
    \item soddisfare le richieste inviate dai client distinguendole da quelle dei nodi
    \item riconoscere se e quando viene aggiunto un nodo nella rete
    \item implementare il cambio di configurazione secondo i criteri del protocollo RAFT
    \item riconoscere se e quando un nodo non e' piu' attivo
    \item permettere il soddisfaciemento di una o piu' delle richieste citate in contemporanea 
        senza implementare spinlock
\end{itemize}
% Sezione vuota per scaletta
%TODO: ingloberei questa sotto sezione alla sezione Obiettivi

\subsubsection{Struttura del codice\\}
Il codice e' stato strutturato per essere diviso in due fasi: 
\begin{itemize}
    \item \textbf{Inizializzazione}: dove vengono caricati tutti gli ip scoperti dal daemon
        \textbf{discovery.service} (quello del nodo stesso e quelli dei nodi attualmente presenti 
        nella rete), la directory dove salvare i dati del filesystem distribuito. Fatto 
        il nodo tenta di instanziare una connessione con gli altri nodi scoperti dal dameon e 
        ifine instanzia una goroutine per l'accettazione di nuove connessioni in ingresso che 
        verranno poi smistate in base all'ip di provenienza. Usando questa struttura si garantisce
        che ogni nodo creera' una connessione, se possibile, con ogni altro nodo nella rete. 
        E' importante ricordare che instaurare una connessione non implica appartenere 
        alla stessa configurazione. Inoltre in questa fase fengono instanziati
        dei clock necessari per l'implementazione del protocollo RAFT.
        E' importante sottolineare che per ogni connessione accettata verra' creata una 
        goroutine responsabile di ricevere i messaggi dal nodo di riferimento.
    \item \textbf{Loop}: In questa fase viene eseguito un loop con una chiamata bloccante che 
        rilascia la \textbf{CPU} fino a che non si verifica una delle tre condizioni:
        \begin{itemize}
            \item il nodo riceve un nuovo messaggio
            \item scatta il timer responsabile dell'elezione di un nuovo leader
            \item scatta il timer che indica di dover inviare un nuovo hearthbit
        \end{itemize}
        E' importante puntualizzare che ogni qualvolta si dovesse verificare una delle condizioni
        verra' creata una goroutine che continuera' la computazione liberando il main loop. 
        Facendo cio' si fornisce un miglior tempo di risposta garentendo inoltre la disponibilita' 
        del servizio.
\end{itemize}
La spiegazione del codice continuera' indagando i tre casi del loop appena citati spiegando i moduli 
utilizzati man mano.

\subsubsection{il nodo riceve un nuovo messaggio\\}
\subsubsection{scatta il timer responsabile dell'elezione di un nuovo leader\\}
\subsubsection{scatta il timer che indica di dover inviare un nuovo hearthbit\\}

Il progetto è composto da numerosi moduli di seguito esposti:
\begin{itemize}
  \item node: definisce la struttura di un nodo Raft, quindi contenente le informazioni riguardanti la connessione al node e il suo indirizzo. Quest'ultimo è rappresentanto grazio a un sottomodulo di node, chiamato address, 
    che contiene la string di numeri separati dal punto (IP) e la porta di riferimento.

  \item localFs: definisce l'astrazione del file system locale e le operazioni di scrittura e lettura su di esso. Grazie a questo modulo, salviamo i file localmente sui vari nodi.
 
  \item raft\_log: definisce il sistema di log di Raft, in cui la struttura LogInstance rappresenta una singola voce nel log di Raft. Il pacchetto importa anche una codifica basata su protobuf per serializzare i messaggi 
    di log per la trasmissione in rete. Una caratteristica chiave di questo pacchetto è il canale ReturnValue, che supporta operazioni asincrone come la ricezione di conferme di commit per le voci del log. \\
    Contiene l'implementazione principale del log per Raft, utilizzando un sync.RWMutex per gestire l'accesso concorrente ai log. Questo file definisce la struttura logEntryImp, che gestisce un elenco di voci di log, la 
    dimensione del log e l'indice di commit. \\
    \'E principalmente costituito da LogEntrySlave e LogEntryMaster. Entrambi condividono lo stesso log, ma con permessi diversi: il LogEntrySlave ha solo permessi di lettura, mentre il LogEntryMaster ha sia permessi di scrittura 
    che di lettura. Il compito del master è notificare agli slave quando vengono aggiunte nuove entry. Gli slave, invece, notificano al master quando hanno confermato le nuove voci, e, al ricevimento di tutte le conferme, 
    il master procede con il commit ufficiale delle voci.
  
  \item clusterMetadata: definisce i metadati relativi al cluster e al nodo stesso, come il proprio indirizzo IP e quello del leader (sia publici che privati), il term per tracciare la validità della leadership, il ruolo 
    che ricopre, le informazioni sui timer (election e heartbeat) e i dettagli 
    sull'elezioni come il quorum, il nodo per cui ha votato e se ha il diritto di voto o meno, 

  \item rpcs: definisce un'interfaccia generica Rpc, utilizzata in tutto il progetto per astrarre la logica delle varie chiamate RPC nel sistema Raft. Nonostante il nome sia fuorviante, in quanto è acronimo delle Remote Procedure
    Call, ci teniamo a specificare che non abbiamo progettato questo modulo per lavorare in sincrono, bensì in asincrono.
    Il sistema delle rpc è stato progettate per alleggerire il carico di lavoro
    componente centrale del nodo. Questo è stato fatto implementando il comportamento di risposta in ricezione di una rpc all'interno delle rpc stesse. Per fare ciò, il modulo fa uso del clusterMetadata 
    per le informazioni sul nodo, i metadati del cluster (serve in particolare per conoscere il numero di nodi nelle configurazioni) e informazioni sul mittente. Necessitavamo di un sistema il più modulare e scalabile possibile,
    in quanto,
    nell'eventualità di aggiungere un nuovo tipo di rpc, non avremo dovuto apportare troppe modifiche. A dimostrazione di ciò, oltre alle rpc definite dal protocollo -- AppendEntryRpc, AppendResponse, RequestVoteRPC, 
    RequestVoteResponse -- ne troviamo altre, come l'rpc utilizzato per notificare ai noti che hanno diritto di voto e quelle che definiscono i messaggi che possiamo ricevere dal client. La ragione dietro a questo tipo di 
    progettazione è stata 
    fatta a frutto dei futuri cambiamenti che ci sarebbero stati e che sono stati fatti, difatti il modulo è stato pensando fin dall'inizio in questo modo e, a fronte dei cambiamenti che sono stati fatti, non è per nulla cambiato. 
    La scalabilità è inoltre facilitata dall'utilizzo di uno script e di un rpc template che utiliziamo per generare nuove RPC (garantisce inoltre di evitare eventuali errori di scrittura di nuove rpc). 
    Utiliziamo però un modulo esterno ad rpc per convertire le classi: quando un messaggio arriva, il suoo tipo viene controllato, viene estratto il payload e convertito in base al tipo. Questo rappresenta un 
    debito tecnico (limite), in quanto ad ogni aggiunta di una nuova rpc, bisogna aggiungere un voce nel modulo di conversione. 
    
  \item confPool: gestisce le configurazioni del sistem. Tiene traccia della configurazione principale, che rappresenta lo stato attuale del sistema, e di una o più configurazioni temporanee che potrebbero essere in fase di 
    valutazione o preparazione per diventare la nuova configurazione principale. Inoltre, si assicura che tutti i nodi del sistema siano d'accordo sulla configurazione corrente. Questo è fondamentale per evitare 
    incoerenze e garantire un funzionamento corretto del sistema. \'E l'unico modulo che ha subito numerose ristrutturazioni: inizialmente abbiamo provato a rendere il gestore dei log indipentende dal gestore delle configurazioni, 
    ma tutto cio' si e' rivelato alquanto fallimentare. In particolar modo il cambio di configurazione risultava troppo complesso da implementare e per questo abbiamo deciso di cambiare approccio usando un altro design. Abbiamo 
    quindi deciso di avere un modulo delle configurazioni il quale, al suo interno, contenesse il gestore dei log su cui avere controllo. Così facendo, divenne più semplice gestire molteplici configurazioni, infatti per aggiornare
    la disposizione dei nodi, è sufficente instanziare una nuova configurazione, aspettare la migrazione dei nodi per poi eliminare quella vecchia in favore della nuova. 
    Col precedente approccio non sarebbe stato possibile poiche' la migrazione veniva 
    considerata, erroneamente, come una vera e propria disposizione (adesso è rappresentata dalla coesistenza di due configurazioni). 
  
  \item server: è la componente centrale che viene eseguito su ciascun nodo. Definisce la struttura e il loop principale del lavoro del nodo. Fa utilizzo dei moduli sopra descritti per restare in ascolta di messaggi e connessioni 
    in arrivo, e conseguentemente gestirle.

\end{itemize}


\subsubsection{Funzionamento}
% Sezione vuota per scaletta
All'avvio, il nodo legge la configurazione, che include l'indirizzo IP dei nodi pesenti e del nodo stesso. Viene quindi creato un server Raft che rappresenta il nodo stesso. Questa configurazione è fondamentale per definire la 
rete di nodi Raft che formeranno il cluster.
Una volta configurato, il server avvia il nodo Raft, che si mette in ascolto per comunicazioni e richieste. A questo punto, ogni nodo può interagire con gli altri nel cluster, comunicando attraverso RPCs (Remote Procedure Calls).

\textbf{Comunicazione tra Nodi}\\
Una parte cruciale del flusso è la comunicazione tra nodi, che avviene principalmente attraverso due fasi: replicazione del log e elezioni del leader. Queste interazioni vengono gestite tramite vari tipi di RPC definiti 
nel progetto.
Quando un nodo leader deve replicare le voci del log, invia un'RPC AppendEntryRpc ai nodi follower. I follower, una volta ricevuta la richiesta, e, se la richiesta è valida e il log viene replicato correttamente, i follower 
rispondono confermando la replica.
Durante il processo di elezione del leader, i nodi inviano una richiesta di voto con l'RPC RequestVoteRPC. I nodi che ricevono questa richiesta processano la decisione di votare o meno per il candidato leader, assicurandosi 
che i requisiti per il voto (come la consistenza del log) siano soddisfatti.

\textbf{Processo di elezione}\\
Il sistema Raft monitora costantemente la presenza di un leader attivo. Quando un nodo non riceve messaggi di "heartbeat" (AppendEntry) dal leader entro un determinato timeout, questo evento segnala che il leader potrebbe essere 
inaccessibile o non funzionante. Ogni nodo follower mantiene un proprio timer, e se scade senza ricevere comunicazioni dal leader, quel nodo considera il leader inattivo e transita in uno stato di candidate.
In questo stato, il nodo invia richieste di voto agli altri nodi del cluster attraverso l'RPC RequestVoteRPC. Il nodo candidato cercherà di ottenere voti sufficienti per diventare leader. Durante questa fase, 
ogni nodo che riceve una richiesta di voto valuta se concedere il proprio voto al candidato, basandosi sulla propria situazione di stato e log.
Se il candidato riceve voti dalla maggioranza dei nodi, assume il ruolo di leader e comincia immediatamente a inviare messaggi AppendEntryRpc ai follower per confermare la propria leadership e mantenere i log sincronizzati. 
Se invece nessun candidato ottiene la maggioranza, viene avviato un nuovo ciclo elettorale fino a quando non viene selezionato un leader. 

\textbf{Replicazione dei Log, Commit e applicazione allo stato della macchina}\\
Uno dei flussi operativi principali riguarda la gestione del log, che è il meccanismo attraverso cui Raft garantisce la consistenza dei dati nei nodi. Il log è gestito da due componenti separate: una chiamata leader e l'altra 
chiamata slave e, come descritto nel paragrafo precedente, la prima ha permessi di scrittura, mentre la seconda di sola lettura.
Quando un leader riceve un nuovo comando, lo aggiunge al suo log interno e, a questo punto, informa le configurazioni attive che ha aggiunto una nuova voce al registro. Dopodiché, ogni configurazione invia un messaggio 
AppendEntryRPC ai nodi corrispondenti.
I follower ricevono questo messaggio, lo aggiungono al loro log, e rispondono al leader. 
Quando il leader riceve una risposta AppendEntryResponse da ciascun nodo, aggiorna i rispettivi indici: nextIndex e matchIndex, mantenendo così allineati i nodi con lo stato corrente del registro.
attraverso canali asincroni (applyC e NotifyAppendEntryC) definiti nelle implementazioni del log di leader e follower. Il leader invia un commit alla maggior parte dei nodi, e i follower seguono l'ordine di commit. 
Quando le voci vengono committate, esse vengono applicate allo stato condiviso del sistema.
Il canale ReturnValue nel log permette di gestire i risultati delle operazioni in maniera asincrona, in modo che il sistema possa continuare ad operare senza attendere che ogni operazione venga completata in modo sincrono.

\textbf{Gestione delle configurazioni}\\
L'aspetto più importante dell'intero cluster è la gestione delle configurazioni. Il sistema prevede due configurazioni distinte: quella corrente e quella nuova. La presenza simultanea di entrambe indica che il cluster si 
trova in uno stato di transizione da una configurazione all'altra.\\
Quando il leader riceve una richiesta da un client per applicare una nuova configurazione, che include un elenco di nodi, aggiorna l'elenco, imposta la nuova configurazione e informa i nuovi nodi che non sono ancora autorizzati
a votare. Il leader inizia quindi a replicare il proprio log, aggiornando i nuovi nodi follower.\\
Durante questa procedura, potrebbe accadere che uno dei nuovi nodi non venga trovato a causa della sua assenza nella subnet. In tal caso, le informazioni su quel nodo vengono memorizzate e aggiornate in seguito. Infine, 
il sistema esegue controlli periodici per verificare se il log di ciascun nodo è sincronizzato; quando un nodo risulta aggiornato, il leader gli concede il diritto di voto.

\subsubsection{Limiti}
% Sezione vuota per scaletta
Anche se il nostro sistema funziona come descritto nel documento (citare documento), presenta un grave difetto. La Figura 3 illustra la parte del sistema principalmente coinvolta nel processo di \textit{commit} degli indici.
\\
\begin{lstlisting}[language=Go]
func (c *commonMatchImp) 
    checkUpdateNewMatch(sub *substriber) { 
  var halfNodeNum = c.numNodes/2 
  for c.run { 
    var newMatch = <-sub.Snd 
    if newMatch > c.commonMatchIndex && 
        sub.Trd < newMatch { 
      c.numStable++ 
      if c.numStable > uint(halfNodeNum) {
        if c.commitEntryC != nil {
          c.commitEntryC <- c.commonMatchIndex
        } 
        c.commonMatchIndex++
        c.numStable = 1
      } 
    } 
    sub.Trd = newMatch
  } 
}
\end{lstlisting}
Il problema deriva da una sezione critica del codice a cui accedono contemporaneamente più Goroutine senza alcun meccanismo di controllo dell'accesso. In particolare, è possibile che una Goroutine 
superi il controllo iniziale ed esegua le operazioni previste, e che subito dopo una seconda Goroutine superi lo stesso controllo. A questo punto, però, la prima Goroutine ha già modificato i valori condivisi all'interno del 
corpo dell'istruzione if, il che significa che la seconda Goroutine opera su valori non aggiornati, invalidando di fatto la condizione del guard. Anche se abbiamo eseguito numerosi test (che riconosciamo essere insufficienti 
per dimostrare l'assenza di bug) e abbiamo trovato difficile riprodurre le condizioni necessarie per far emergere questa condizione di race, non abbiamo ancora osservato questo problema nella pratica.
